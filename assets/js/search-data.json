{
  
    
        "post0": {
            "title": "Tedium Free MLE",
            "content": "Introduction . Maximum likelihood estimation has the dubious honor of being difficult for humans and machines alike (difficult for machines at least in the naïve formulation that doesn&#39;t use log-likelihood). . MLE is challenging for humans because it requires the multiplication of $n$ likelihood expressions, which is time consuming and error prone - this is the tedium part we&#39;re trying to avoid. Fortunately, computers are very good at repeated multiplication, even repeated symbolic multiplication. . Problem Formulation and Example . MLE estimates parameters of an assumed probability distribution, given data $x_i$ observed independently from the same distribution. If that distribution has probability function $f( cdot)$, then the likelihood of $x_i$ is $f(x_i)$. . As the $x_i$s are independent, the likelihood of all $x_i$s will be the product of their individual likelihoods. In mathematical notation, the product will be: . $$ prod_{i=1}^{n} f(x_i)$$ . Probability functions (mass functions or density functions) like our $f( cdot)$ typically have parameters. For instance, the Gaussian distribution has parameters $ mu$ and $ sigma^2$, and the Poisson distribution has rate parameter λ. We use MLE to estimate these parameters, so they are the unknowns in the expression and they will appear in each $f(x_i)$ term. We can restate the problem as an equality with the generic parameter $ theta$: . $$L( theta) = prod_{i=1}^{n} f(x_i)$$ . The expression $L( theta)$ is the likelihood. In order to find the MLE it is necessary to maximize this function, or find the value of $ theta$ for which $L( theta)$ is as large as possible. This process is probably easier to show than to describe. In particular, we&#39;ll be demonstrating the usefulness of the sympy module in making these symbolic calculations. . Example . Say we observed values $[3,1,2]$ generated from a Poisson. What is likelihood function of λ? . Importing the necessities and setting up some symbols and expressions: . from sympy.stats import Poisson, density, E, variance from sympy import Symbol, simplify from sympy.abc import x lambda_ = Symbol(&quot;lambda&quot;, positive=True) f = Poisson(&quot;f&quot;, lambda_) density(f)(x) . $ displaystyle frac{ lambda^{x} e^{- lambda}}{x!}$ sympy gives us a representation of the Poisson density to work with in the Poisson() object, keeping track of all of the terms internally. . The likelihood expression is the product of the probability function evaluated at these three points: . L_ = density(f)(3) * density(f)(1) * density(f)(2) L_ . $ displaystyle frac{ lambda^{6} e^{- 3 lambda}}{12}$ That&#39;s our expression for the likelihood $L( theta)$ 🙂 In order to maximize the expression, we&#39;ll take the derivative expression and then solve for the value of parameter $ lambda$ where the derivative expression is equal to 0. This value of $ lambda$ will maximize the likelihood. . Finding the derivative using sympy: . from sympy import diff dL_ = diff(L_, lambda_) dL_ . $ displaystyle - frac{ lambda^{6} e^{- 3 lambda}}{4} + frac{ lambda^{5} e^{- 3 lambda}}{2}$ Setting the derivative $ frac{dL}{d theta}$ equal to zero: . from sympy import Eq dLeqz = Eq(dL_, 0) dLeqz . $ displaystyle - frac{ lambda^{6} e^{- 3 lambda}}{4} + frac{ lambda^{5} e^{- 3 lambda}}{2} = 0$ And finally, solving the equation for $ lambda$: . from sympy import solve solve(dLeqz, lambda_) . [2] . And that&#39;s our answer! . Complications . There is a slight wrinkle with this approach. It is susceptible to numerical instability, which (luckily) did not affect us in this example. This is how MLE can become difficult for computers too. . Likelihoods are usually very small numbers and computers simply can&#39;t track numbers that are too small or too large. Multiplying very small numbers together repeatedly makes very VERY small numbers that can sometimes disappear completely. Without getting too distracted by the minutiae of numerical stability or underflow, we can still appreciate some bizarre behavior that results when floats are misused: . 6.89 + .1 . 6.989999999999999 . (0.1)**512 . 0.0 . In the second scenario, we can imagine having 512 data points and finding that the likelihood evaluates to 0.1 (times our parameter) for every single one. Then our product would look like $g( theta) cdot (0.1)^{512}$. The computer just told us that one of those terms is zero, and we&#39;re left unable to find the parameters for our MLE. . Solution . What do we do instead? Is there any way to make these numbers bigger, without changing the problem or solution? Is there an equivalent problem with bigger numbers? . Adding a number and multiplying by a number don&#39;t fix the problem - they just add terms to the expression, which ends up zero anyhow. However these functions do have one property that we will need to be sure we are solving an equivalent problem: they preserve the order of the input in the output. We call these functions monotonic. . The monotonic functions also include the log function. The log function has some very nice properties, not least of which that it makes our calculations immune to the problems we saw above. Calculating the log likelihood: . from sympy import log _ = simplify(log(L_)) _ . $ displaystyle - 3 lambda + 6 log{ left( lambda right)} - log{ left(12 right)}$ And then taking the derivative as before: . d_ = diff(_, lambda_) d_ . Setting equal to zero: . _ = Eq(_, 0) _ . $ displaystyle -3 + frac{6}{ lambda} = 0$ And solving: . from sympy import solve solve(_, lambda_) . [2] . The two solutions agree! Which is necessary, but not sufficient to show these methods are equivalent in general. .",
            "url": "https://nshea3.github.io/blog/2021/10/19/Tedium-Free-MLE.html",
            "relUrl": "/2021/10/19/Tedium-Free-MLE.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "The birthday problem here and elsewhere",
            "content": "Introduction . The birthday problem is a classic probabilistic puzzle concerning the probability of any two people among $n$ people in a group sharing the same birthday. . In a group of $n$ people, what is the probability that two of these people share a birthday? . The intuition around the puzzle is very misleading, so much so that it is sometimes called the birthday paradox, even though there is ultimately no paradox in the problem or its solution. . Birthday Problem Here . In fact, given some reasonable assumptions, there is a simple closed form expression for the probability:$p(n)= frac{365 !}{365^{n}(365-n) !}$ . under the assumption that all $n$ birthdays are drawn uniformly at random from 365 possible dates. . However, this model leads to some very surprising results when we consider the probabilities of finding shared birthdays in groups of different size $n$. . We can ask more interesting questions: for instance, what group size $n$ would result in a 50% chance of a shared birthday within the group? As an upper bound on $n$, a party of size $n=366$ must have two individuals that share the same birthday, for $p=1.0$, since there are only 365 unique birthdays to be assigned (by the famous pigeonhole principle). The answer for $p=0.5$, it turns out, is only $n=23$. . There is &gt;50% probability that among a group of 23 people, 2 share a birthday! . This is such a surprising conclusion that the whole problem is sometimes called a paradox! . Fortunately, verifying this result can be done with simulation just as well as with analytical methods. The code below assembles groups individual by individual, stopping when two individuals share a birthday. This is detected when a list of day numbers differs in size from a set of day numbers, because a set discards duplicates while a list does not: . import random def birthday_sim(n_days=365): dates = [] while len(set(dates)) == len(dates): dates.append(random.randint(1,n_days + 1)) return len(dates) . This code can easily run 100,000 simulations: . sims_100k = [birthday_sim() for x in range(100000)] . The distribution is right-skewed, which we&#39;d expect with a distribution centered at near 20 but taking values up to 365. The peak near 20 confirms the conclusion above that a value in this area corresponds to the 50th percentile, or the approximation of $p=0.5$ in this sample. . import matplotlib.pyplot as plt hist100k = plt.hist(sims_100k, bins = 89) . The median represents the 50th percentile/2nd quantile/50% probability and sits exactly at 23: . import statistics statistics.median(sims_100k) . 23.0 . The CDF gives a nice view of the problem: now we can read off the probabilities and the corresponding group size $n$. Here we find a few more surprising conclusions: we hit 80% probability of having a shared birthday just under $n=40$, and a 90% probability just over $n=40$. . hist100k = plt.hist(sims_100k, bins = 89, cumulative=True, density=True) . In group sizes over $n=60$ finding a shared birthday should be a near certainty. The table puts the probability at 99.4% where $n=60$. . This phenomenon is likely so unintuitive partially because the question is misunderstood. &quot;Find any two people with the same birthday&quot; can so easily be misunderstood as &quot;Select one person, and find another person with the same birthday&quot;, or &quot;find one person in the group that shares MY birthday&quot;. Neither of these is the correct interpretation. There is no need to match a specific person, or a specific date. These interpretations lack the combinatorial characteristic that makes these probabilities so high for low $n$; there are just so many different ways of combining $n$ people into groups of two! . Birthday Problem Elsewhere . Another advantage of the simulation method is flexibility. If we imagine we&#39;re meeting groups of $n$ individuals born on a different planet with a different year length, there is no need to re-derive the equation above because the simulation method adapts seamlessly to the new problem. We simply have to change the n_days parameter. . For example, a single year lasts 88 earth days on Mercury (though a Mercury day lasts 2 Mercury years!): . mercury_sims_100k = [birthday_sim(88) for x in range(100000)] hist100k = plt.hist(mercury_sims_100k, bins = 39, cumulative=True, density=True) . statistics.median(mercury_sims_100k) . 12.0 . So the corresponding value for 50% probability on Mercury? $n=12$. . Clearly, the simulation technique is well suited to this problem - in fact, this class of problems - and might even be considered an improvement on the available analytical methods! .",
            "url": "https://nshea3.github.io/blog/2021/01/19/Birthday-problem-here-and-elsewhere.html",
            "relUrl": "/2021/01/19/Birthday-problem-here-and-elsewhere.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Mathematics of Drilling Intercepts",
            "content": "Introduction . Drilling intercepts are a prominent feature of junior mining news releases. They are closely monitored by the mining investment community, and a particularly good intercept can raise the prospects for a project. . As an example, consider this November 10 2020 release from Freegold Ventures: . Freegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit . The market responded with a 3% boost in the share price the next trading day, so clearly this was regarded as a positive signal for the company&#39;s prospects. (This is typical:capital markets tend to treat any news of this sort as good news.) The implications for the economic, geological, and engineering variables surrounding the project are much less clear. Is this a good geological result? Is it a good engineering result? Intercepts are highlights: incomplete data, collected and released selectively, so is it even possible to make an informed judgement using these numbers? . To complicate things even further, the selectively reported drilling intercepts are usually released in a rather complex manner, which can make it difficult to distinguish between truly good numbers and deceptively good results. Drilling intercepts are discussed at great length in other sources (here and here and here) but we&#39;ll take a mathematical perspective and develop a model that describes nested intercept configurations of arbitrary complexity. . We&#39;ll take Great Bear Resources for an extended example. Great Bear Resources is a Canadian junior mining company whose stock gained substantially on announcement of very high grade intercepts at their Dixie project in Ontario. At time of writing, GBR is trading at a $886 million CAD market cap (which is not very bearish at all!) . GBR Price Today by TradingView Here we open up the spreadsheet of drilling results (available on their website), and then filter on Drill Hole to consider a single hole: . import pandas as pd intercepts = pd.read_excel(&#39;drive/My Drive/Projects/posts/data/Great_Bear/lp_drill_hole_composites_all.xlsx&#39;) intercepts[&#39;Record&#39;] = intercepts.index dh = intercepts[intercepts[&#39;Drill Hole&#39;] == &#39;BR-022&#39;] dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | . This is how intercepts are typically presented: a table with a From field describing where they started measuring, a To field describing where they stopped, and a Grade field (called Gold here) that tells us how enriched that interval is with the valuable stuff. From and To are typically measured downhole from the drill collar. . It&#39;s easy to establish a basic understanding of how these tables are read, and many experienced mining investors immediately recognize these grades as very high. The rest of us might need to rely on statistics, since we don&#39;t have the benefit of many years&#39; experience with drilling results. . Of course it is first necessary to determine the true assay values for each separate interval from top to bottom. Unfortunately, each row is not independent - some of the intercepts are contained in others, and the subinterval gold is INCLUDED in the parent interval calculation! So we can&#39;t just use the Gold (g/t) field directly, since intercepts are reported with these &quot;highlights&quot;, or higher grade sections within the longer interval. . Sometimes this convention is used unethically to suggest larger intervals of enrichment than truly exist. This is called &quot;grade smearing&quot; and the method of residual grade calculation applied here will detect any such attempt to disguise poor results. . At first it may seem like the correct interpretation of these intervals is to imagine them intervals stacked on top of one another, but this is very misleading. We can easily visualize this to see the error: . We only have the total grade, INCLUDING the high-grade, child subintervals. Considering it in that way ignores the fact that the high-grade intervals are included in the wider, lower-grade intervals, inflating the grade measured over that length. This has enormous implications for the continuity of the mineralization, which determines the feasibility of the project. | . In order to eliminate this effect we&#39;ll need to do some math with the intercepts. This visualization attempts to show this hierarchical, branching structure: . Plotted side by side, the intercepts show the parent-child overlapping relationship and capture the complexity of the problem. . Parent intervals can have no child intervals, a single child interval, or several child intervals. Child intervals themselves can have no child intervals, a single child interval, or several child intervals. Clearly there is a whole class of related problems we could solve with a general solution to this problem. . So far we have treated the From and To fields in isolation, and we can use a cool feature of Pandas to convert them to intervals: . dh[&#39;Interval&#39;] = dh.apply(lambda x: pd.Interval(x[&#39;From (m)&#39;], x[&#39;To (m)&#39;]), axis=1) dh . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . So the motivation here was to create Interval objects to use them with the pd.Overlaps function and then model the overlap relationship among the different intervals: . import itertools import numpy as np cross_interval = itertools.product(dh.Interval,dh.Interval) overlap_matrix = np.array([interval[0].overlaps(interval[1]) for interval in cross_interval]) intersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval]) ns = int(np.sqrt(overlap_matrix.shape[0])) . Here we see the overlaps: if a pixel is white, it means that the interval on the x-axis and the interval on the y-axis overlap. . Overlap is symmetric: so each &#39;child&#39; overlaps with its parent and vice versa. It should become clear that we are actually interested in the &quot;contains&quot; relationship, which is not symmetric and will help us identify parent intervals and child intervals and start reducing the intervals. . Fortunately this is also supported in Python: . from sympy import Interval dh[&#39;Interval_obj&#39;] = dh.apply(lambda x: Interval(x[&#39;From (m)&#39;], x[&#39;To (m)&#39;]), axis=1) cross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj) contain_matrix = np.array([interval[0].is_proper_superset(interval[1]) for interval in cross_interval]) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Now we can pull out a tree . Of the machine-intelligible formats, a tree data structure is clearly the most suited to representing the intervals. . for i, col in enumerate(contain_matrix_sq.T): if ~np.any(col): all_str_to_node[str(dh[&#39;Record&#39;].values[i])].parent = root else: all_str_to_node[str(dh[&#39;Record&#39;].values[i])].parent = all_str_to_node[str(dh[&#39;Record&#39;].values[::-1][np.argmax(col[::-1])])] print(RenderTree(root, style=AsciiStyle()).by_attr()) . DH |-- 95 | +-- 96 |-- 97 |-- 98 | +-- 99 |-- 100 |-- 101 | +-- 102 | |-- 103 | |-- 104 | +-- 105 | +-- 106 +-- 107 . Now we are really getting somewhere- we can actually start looking at the global picture (since we now know which intervals are not &quot;child&quot; intervals) . These are the direct children. We can go ahead and plot them and have a totally accurate picture of the log: . dh_prime.dtypes . Drill Hole object Unnamed: 1 object From (m) float64 To (m) float64 Width (m) float64 Gold (g/t) float64 Record int64 Interval interval[float64] Interval_obj object dtype: object . While that is correct, it is not complete: we have left out all of the additional information provided by the smaller sub-intervals! . In order to incorporate that we will have to remove them from the parent intervals and determine the residual grade (whatever is left once we pull out the gold contained in the subinterval) . ((119) * (3.78) - (3) * (131.5)) / (119 - 3) . 0.47689655172413786 . As an example of this kind of calculation, a simpler set of intervals from a Freegold Ventures press release: . Freegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit . We know the gold grade over the whole 119 meters, and the gold grade over 3 meters, but what is the gold grade over the $119 - 3 = 116 m$? . The solution is a simple weighted average calculation, like compositing over a drillhole:$ frac{119 times 3.78-3 times 131.5}{119-3} = 0.477 g/t$ . Credit to https://twitter.com/BrentCo77759016/status/1326183861722599424 and . So now we have to do this, but with every single subinterval until we get the residual grade at every point along the drillhole . Fortunately, the tree data structure we selected has specialized methods that make a traversal very simple. . levelord_nodes = [(node.name, node.children) for node in LevelOrderIter(root)] levelord_nodes . [(&#39;DH&#39;, (Node(&#39;/DH/95&#39;), Node(&#39;/DH/97&#39;), Node(&#39;/DH/98&#39;), Node(&#39;/DH/100&#39;), Node(&#39;/DH/101&#39;), Node(&#39;/DH/107&#39;))), (&#39;95&#39;, (Node(&#39;/DH/95/96&#39;),)), (&#39;97&#39;, ()), (&#39;98&#39;, (Node(&#39;/DH/98/99&#39;),)), (&#39;100&#39;, ()), (&#39;101&#39;, (Node(&#39;/DH/101/102&#39;),)), (&#39;107&#39;, ()), (&#39;96&#39;, ()), (&#39;99&#39;, ()), (&#39;102&#39;, (Node(&#39;/DH/101/102/103&#39;), Node(&#39;/DH/101/102/104&#39;), Node(&#39;/DH/101/102/105&#39;))), (&#39;103&#39;, ()), (&#39;104&#39;, ()), (&#39;105&#39;, (Node(&#39;/DH/101/102/105/106&#39;),)), (&#39;106&#39;, ())] . nn_np_loi = [(node.name, node.parent) for node in LevelOrderIter(root)] . all_str_to_node . {&#39;100&#39;: Node(&#39;/DH/100&#39;), &#39;101&#39;: Node(&#39;/DH/101&#39;), &#39;102&#39;: Node(&#39;/DH/101/102&#39;), &#39;103&#39;: Node(&#39;/DH/101/102/103&#39;), &#39;104&#39;: Node(&#39;/DH/101/102/104&#39;), &#39;105&#39;: Node(&#39;/DH/101/102/105&#39;), &#39;106&#39;: Node(&#39;/DH/101/102/105/106&#39;), &#39;107&#39;: Node(&#39;/DH/107&#39;), &#39;95&#39;: Node(&#39;/DH/95&#39;), &#39;96&#39;: Node(&#39;/DH/95/96&#39;), &#39;97&#39;: Node(&#39;/DH/97&#39;), &#39;98&#39;: Node(&#39;/DH/98&#39;), &#39;99&#39;: Node(&#39;/DH/98/99&#39;)} . for node, parent in nn_np_loi[::-1][:-1]: print(node) for child in all_str_to_node[node].children: print(child) . 106 105 Node(&#39;/DH/101/102/105/106&#39;) 104 103 102 Node(&#39;/DH/101/102/103&#39;) Node(&#39;/DH/101/102/104&#39;) Node(&#39;/DH/101/102/105&#39;) 99 96 107 101 Node(&#39;/DH/101/102&#39;) 100 98 Node(&#39;/DH/98/99&#39;) 97 95 Node(&#39;/DH/95/96&#39;) . dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record Interval Interval_obj . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | (110.0, 116.1] | Interval(110.000000000000, 116.100000000000) | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | (111.4, 113.1] | Interval(111.400000000000, 113.100000000000) | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | (274.0, 299.0] | Interval(274.000000000000, 299.000000000000) | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | (432.9, 439.0] | Interval(432.900000000000, 439.000000000000) | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | (432.9, 435.7] | Interval(432.900000000000, 435.700000000000) | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | (445.0, 452.0] | Interval(445.000000000000, 452.000000000000) | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | (461.6, 512.0] | Interval(461.600000000000, 512.000000000000) | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | (471.0, 512.0] | Interval(471.000000000000, 512.000000000000) | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | (471.0, 478.0] | Interval(471.000000000000, 478.000000000000) | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | (490.0, 491.0] | Interval(490.000000000000, 491.000000000000) | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | (505.2, 508.75] | Interval(505.200000000000, 508.750000000000) | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | (506.2, 506.7] | Interval(506.200000000000, 506.700000000000) | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | (600.0, 620.0] | Interval(600.000000000000, 620.000000000000) | . cross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj) intersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval]) intersect_matrix . array([Interval(110.000000000000, 116.100000000000), Interval(111.400000000000, 113.100000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(111.400000000000, 113.100000000000), Interval(111.400000000000, 113.100000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(274.000000000000, 299.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(432.900000000000, 439.000000000000), Interval(432.900000000000, 435.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(432.900000000000, 435.700000000000), Interval(432.900000000000, 435.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(445.000000000000, 452.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(461.600000000000, 512.000000000000), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 478.000000000000), Interval(490.000000000000, 491.000000000000), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 478.000000000000), Interval(490.000000000000, 491.000000000000), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(471.000000000000, 478.000000000000), Interval(471.000000000000, 478.000000000000), Interval(471.000000000000, 478.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(490.000000000000, 491.000000000000), Interval(490.000000000000, 491.000000000000), EmptySet(), Interval(490.000000000000, 491.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(505.200000000000, 508.750000000000), Interval(505.200000000000, 508.750000000000), EmptySet(), EmptySet(), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(506.200000000000, 506.700000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), Interval(506.200000000000, 506.700000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(600.000000000000, 620.000000000000)], dtype=object) . dh[&#39;grade_len&#39;] = dh[&#39;Gold (g/t)&#39;] * dh[&#39;Width (m)&#39;] . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record Interval Interval_obj grade_len . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | (110.0, 116.1] | Interval(110.000000000000, 116.100000000000) | 15.982 | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | (111.4, 113.1] | Interval(111.400000000000, 113.100000000000) | 13.464 | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | (274.0, 299.0] | Interval(274.000000000000, 299.000000000000) | 4.750 | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | (432.9, 439.0] | Interval(432.900000000000, 439.000000000000) | 24.705 | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | (432.9, 435.7] | Interval(432.900000000000, 435.700000000000) | 22.904 | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | (445.0, 452.0] | Interval(445.000000000000, 452.000000000000) | 2.870 | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | (461.6, 512.0] | Interval(461.600000000000, 512.000000000000) | 89.712 | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | (471.0, 512.0] | Interval(471.000000000000, 512.000000000000) | 85.690 | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | (471.0, 478.0] | Interval(471.000000000000, 478.000000000000) | 16.590 | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | (490.0, 491.0] | Interval(490.000000000000, 491.000000000000) | 8.150 | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | (505.2, 508.75] | Interval(505.200000000000, 508.750000000000) | 52.895 | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | (506.2, 506.7] | Interval(506.200000000000, 506.700000000000) | 50.240 | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | (600.0, 620.0] | Interval(600.000000000000, 620.000000000000) | 10.400 | . from sympy import Union import functools resid_grades, resid_len = {}, {} for node in levelord_nodes[1:]: parent_interval = dh[dh[&#39;Record&#39;] == float(node[0])] child_names = [child.name for child in node[1]] child_intervals = [dh[dh[&#39;Record&#39;] == float(child)] for child in child_names] new_interval_obj = parent_interval.Interval_obj.values[0] - Union([child.Interval_obj.values[0] for child in child_intervals]) l_child_int = Union([intv[&#39;Interval_obj&#39;].values[0] for intv in child_intervals])._measure lg_child_int = [dh.loc[int(child_name)][&#39;grade_len&#39;] for child_name in child_names] lg_total_int = parent_interval.grade_len.values[0] residual_grade = (lg_total_int - sum(lg_child_int)) / (new_interval_obj._measure) resid_grades[node[0]] = residual_grade resid_len[node[0]] = new_interval_obj print(&quot;Interval:&quot;) print(node[0]) print(&quot;Length x Grade:&quot;) print(lg_total_int - sum(lg_child_int)) print(&quot;Residual Grade:&quot;) print(residual_grade) . Interval: 95 Length x Grade: 2.517999999999999 Residual Grade: 0.572272727272726 Interval: 97 Length x Grade: 4.75 Residual Grade: 0.190000000000000 Interval: 98 Length x Grade: 1.801000000000002 Residual Grade: 0.545757575757574 Interval: 100 Length x Grade: 2.8699999999999997 Residual Grade: 0.410000000000000 Interval: 101 Length x Grade: 4.022000000000006 Residual Grade: 0.427872340425534 Interval: 107 Length x Grade: 10.4 Residual Grade: 0.520000000000000 Interval: 96 Length x Grade: 13.464 Residual Grade: 7.92000000000005 Interval: 99 Length x Grade: 22.903999999999996 Residual Grade: 8.17999999999997 Interval: 102 Length x Grade: 8.055000000000007 Residual Grade: 0.273514431239389 Interval: 103 Length x Grade: 16.59 Residual Grade: 2.37000000000000 Interval: 104 Length x Grade: 8.15 Residual Grade: 8.15000000000000 Interval: 105 Length x Grade: 2.654999999999994 Residual Grade: 0.870491803278683 Interval: 106 Length x Grade: 50.24 Residual Grade: 100.480000000000 . Check these solutions: 95 should be easy to validate . includes 96 . 95 extends from 110.00 m to 116.10 m (l = 6.10 m) . 96 is the interval from 111.40 to 113.10 m (l = 1.70 m) . Larger interval . (6.10 m)(2.62 g/t) = (1.7 m)(7.92 g/t) + (4.4 m)(x g/t) . Rearranging terms, we get: . $x = frac{(6.10 m)(2.62 g/t) - (1.7 m)(7.92 g/t)}{ 4.4 m }$ . So the residual grade is 0.5723 g/t, which matches the value found above! . ((6.10 * 2.62) - (1.7)*(7.92)) / (4.4) . 0.5722727272727269 . resid_len . {&#39;100&#39;: Interval(445.000000000000, 452.000000000000), &#39;101&#39;: Interval.Ropen(461.600000000000, 471.000000000000), &#39;102&#39;: Union(Interval.open(478.000000000000, 490.000000000000), Interval.open(491.000000000000, 505.200000000000), Interval.Lopen(508.750000000000, 512.000000000000)), &#39;103&#39;: Interval(471.000000000000, 478.000000000000), &#39;104&#39;: Interval(490.000000000000, 491.000000000000), &#39;105&#39;: Union(Interval.Ropen(505.200000000000, 506.200000000000), Interval.Lopen(506.700000000000, 508.750000000000)), &#39;106&#39;: Interval(506.200000000000, 506.700000000000), &#39;107&#39;: Interval(600.000000000000, 620.000000000000), &#39;95&#39;: Union(Interval.Ropen(110.000000000000, 111.400000000000), Interval.Lopen(113.100000000000, 116.100000000000)), &#39;96&#39;: Interval(111.400000000000, 113.100000000000), &#39;97&#39;: Interval(274.000000000000, 299.000000000000), &#39;98&#39;: Interval.Lopen(435.700000000000, 439.000000000000), &#39;99&#39;: Interval(432.900000000000, 435.700000000000)} . Interval(445.000000000000, 452.000000000000).end . 452.000000000000 . dh[&#39;Record&#39;].astype(str).map(resid_len) dh[&#39;Record&#39;].astype(str).map(resid_grades) . 95 0.572272727272726 96 7.92000000000005 97 0.190000000000000 98 0.545757575757574 99 8.17999999999997 100 0.410000000000000 101 0.427872340425534 102 0.273514431239389 103 2.37000000000000 104 8.15000000000000 105 0.870491803278683 106 100.480000000000 107 0.520000000000000 Name: Record, dtype: object . TODO: Need to split up the non-contiguous segments so that you can actually plot them . Idea: use .args attribute Problem: This is defined for Interval objects as well and it gets us something we don&#39;t want . dh[&#39;Record&#39;].astype(str).map(resid_len) . 95 Union(Interval.Ropen(110.000000000000, 111.400... 96 Interval(111.400000000000, 113.100000000000) 97 Interval(274.000000000000, 299.000000000000) 98 Interval.Lopen(435.700000000000, 439.000000000... 99 Interval(432.900000000000, 435.700000000000) 100 Interval(445.000000000000, 452.000000000000) 101 Interval.Ropen(461.600000000000, 471.000000000... 102 Union(Interval.open(478.000000000000, 490.0000... 103 Interval(471.000000000000, 478.000000000000) 104 Interval(490.000000000000, 491.000000000000) 105 Union(Interval.Ropen(505.200000000000, 506.200... 106 Interval(506.200000000000, 506.700000000000) 107 Interval(600.000000000000, 620.000000000000) Name: Record, dtype: object . def args_extended(interval): if type(interval) == Union: return interval.args else: return interval remaining_interval_grade = pd.DataFrame({&#39;interval&#39; : dh[&#39;Record&#39;].astype(str).map(resid_len), &#39;grade&#39; : dh[&#39;Record&#39;].astype(str).map(resid_grades)}) remaining_interval_grade[&#39;split_intervals&#39;] = remaining_interval_grade.interval.apply(args_extended) rig_exploded = remaining_interval_grade.explode(&#39;split_intervals&#39;) . rig_exploded . interval grade split_intervals . 95 Union(Interval.Ropen(110.000000000000, 111.400... | 0.572272727272726 | Interval.Ropen(110.000000000000, 111.400000000... | . 95 Union(Interval.Ropen(110.000000000000, 111.400... | 0.572272727272726 | Interval.Lopen(113.100000000000, 116.100000000... | . 96 Interval(111.400000000000, 113.100000000000) | 7.92000000000005 | Interval(111.400000000000, 113.100000000000) | . 97 Interval(274.000000000000, 299.000000000000) | 0.190000000000000 | Interval(274.000000000000, 299.000000000000) | . 98 Interval.Lopen(435.700000000000, 439.000000000... | 0.545757575757574 | Interval.Lopen(435.700000000000, 439.000000000... | . 99 Interval(432.900000000000, 435.700000000000) | 8.17999999999997 | Interval(432.900000000000, 435.700000000000) | . 100 Interval(445.000000000000, 452.000000000000) | 0.410000000000000 | Interval(445.000000000000, 452.000000000000) | . 101 Interval.Ropen(461.600000000000, 471.000000000... | 0.427872340425534 | Interval.Ropen(461.600000000000, 471.000000000... | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.open(478.000000000000, 490.000000000000) | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.open(491.000000000000, 505.200000000000) | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.Lopen(508.750000000000, 512.000000000... | . 103 Interval(471.000000000000, 478.000000000000) | 2.37000000000000 | Interval(471.000000000000, 478.000000000000) | . 104 Interval(490.000000000000, 491.000000000000) | 8.15000000000000 | Interval(490.000000000000, 491.000000000000) | . 105 Union(Interval.Ropen(505.200000000000, 506.200... | 0.870491803278683 | Interval.Ropen(505.200000000000, 506.200000000... | . 105 Union(Interval.Ropen(505.200000000000, 506.200... | 0.870491803278683 | Interval.Lopen(506.700000000000, 508.750000000... | . 106 Interval(506.200000000000, 506.700000000000) | 100.480000000000 | Interval(506.200000000000, 506.700000000000) | . 107 Interval(600.000000000000, 620.000000000000) | 0.520000000000000 | Interval(600.000000000000, 620.000000000000) | . rig_exploded[&#39;From&#39;] = rig_exploded.split_intervals.apply(lambda x: x.start).astype(float) rig_exploded[&#39;To&#39;] = rig_exploded.split_intervals.apply(lambda x: x.end).astype(float) rig_exploded[&#39;Width&#39;] = (rig_exploded.To - rig_exploded.From).astype(float) rig_exploded[&#39;grade&#39;] = rig_exploded.grade.astype(float) rig_exploded[&#39;drillhole&#39;] = &#39;BR-022&#39; . rig_exploded[[&#39;From&#39;, &#39;To&#39;, &#39;grade&#39;, &#39;Width&#39;]] . From To grade Width . 95 110.00 | 111.40 | 0.572273 | 1.40 | . 95 113.10 | 116.10 | 0.572273 | 3.00 | . 96 111.40 | 113.10 | 7.920000 | 1.70 | . 97 274.00 | 299.00 | 0.190000 | 25.00 | . 98 435.70 | 439.00 | 0.545758 | 3.30 | . 99 432.90 | 435.70 | 8.180000 | 2.80 | . 100 445.00 | 452.00 | 0.410000 | 7.00 | . 101 461.60 | 471.00 | 0.427872 | 9.40 | . 102 478.00 | 490.00 | 0.273514 | 12.00 | . 102 491.00 | 505.20 | 0.273514 | 14.20 | . 102 508.75 | 512.00 | 0.273514 | 3.25 | . 103 471.00 | 478.00 | 2.370000 | 7.00 | . 104 490.00 | 491.00 | 8.150000 | 1.00 | . 105 505.20 | 506.20 | 0.870492 | 1.00 | . 105 506.70 | 508.75 | 0.870492 | 2.05 | . 106 506.20 | 506.70 | 100.480000 | 0.50 | . 107 600.00 | 620.00 | 0.520000 | 20.00 | . y_axis = alt.Axis( title=&#39;Intercept ID&#39;, offset=5, ticks=False, domain=False ) reqd_cols = [&#39;From&#39;, &#39;To&#39;, &#39;grade&#39;, &#39;Width&#39;, &#39;drillhole&#39;] alt.Chart(rig_exploded[reqd_cols]).mark_bar().encode( alt.X(&#39;From:Q&#39;, scale=alt.Scale(zero=False)), x2=&#39;To:Q&#39;, y=alt.Y(&#39;drillhole:N&#39;, axis=y_axis), color=alt.Color(&#39;grade:Q&#39;, scale=alt.Scale(scheme=&quot;inferno&quot;)), tooltip=[ alt.Tooltip(&#39;Width:Q&#39;, title=&#39;Width&#39;), alt.Tooltip(&#39;grade:Q&#39;, title=&#39;Gold Grade&#39;) ] ).properties(width=800, height=100).configure(background=&#39;#D9E9F0&#39;).interactive() .",
            "url": "https://nshea3.github.io/blog/2020/12/02/Mathematics-of-Drilling-Intercepts.html",
            "relUrl": "/2020/12/02/Mathematics-of-Drilling-Intercepts.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Simple Betting Strategies in Python",
            "content": "Introduction . A student of probability is practically guaranteed to see games of chance during their studies - coin flipping, card drawing, dice games. This usually seems contrived, when in fact games of chance have been central to the development of probability and statistics. Unfortunately, the games studied are often simplified and betting strategies are constrained. . There are good reasons for this: analysis of betting strategies requires an understanding of simulation, which might be too advanced for an intro probability course. But at a certain point it&#39;s useful to analyze simple toy strategies and discover their extreme properties. The martingale system is one such strategy: a simple betting that recommends doubling the value bet with each loss. . It is not a new strategy, as we can gather from this 18th century account: . Casanova’s diary, Venice, 1754:Playing the martingale, and doubling my stakescontinuously, I won every day during the remainder of the carnival. [..] I congratulated myself upon having increased the treasure of my dear mistress [..] . Then, several days later: . I still played on the martingale, but with such bad luck that I was soon left without a sequin. As I shared my property with my mistress, [..] at her request I sold all her diamonds, losing what I got for them; she had now only five hundred sequins by her. There was no more talk of her escaping from the convent, for we had nothing to live on! . Source:Betting systems: how not to lose your moneygambling (G. Berkolaiko) . Casanova&#39;s account is important as it reveals some of the shortcomings of the strategy: the strategy does not consider the remaining bankroll, so the gambler with finite resources exposes themselves to the risk of ruin. In slightly more technical terms martingale betting strategy on a fair (50-50, even odds) game is a sure thing only with infinite capital to absorb long losing streaks with exponentially increasing losses. An unfair (subfair) game is a loss no matter the betting strategy (there is theoretical work to support this). Subfair games include most casino games (poker excepted - the casino will never let you pick their pocket but picking another punter&#39;s pocket is a matter of supreme indifference). . Roulette is one such subfair casino game. Roulette distinguished not only by its but its utility as a litmus for gamblers. Roulette strategies reveal bad gamblers (who favor magical thinking and lucky numbers), smart gamblers (who strictly use bold play on red or black), and smarter gamblers (to be found in the poker room or at a hedge fund). . Roulette, or rather Rou-let&#39;s not . The roulette wheel is likely familiar from any number of appearances in movies and TV. The pockets in the American roulette wheel are labeled with numbers 1 to 36, alternating red and black color. Two special pockets, numbered 0 and 00, are colored green. . . Boil it down to a choice, uniformly at random between the integers 1 to 38, inclusive. (notice that there is a 0 and a 00 that are neither Red nor Black). . There is a commom misconception that the machine needs to be somehow &quot;rigged&quot; or &quot;fixed&quot; (non-uniform random) to rob the player of their advantage. This is not true, the math works out that way for a random choice. The mechanism is completely random, but the house edge is guaranteed through careful construction of the wheel and the possible wagers. If you lose your shirt at the table, your gripe is with probability, not the casino or the gaming control board. . The house edge comes from a restriction on the possible bets and unfavorable payouts on the allowed bets. If I had my way, I would walk into the casino and wager all of my money against all of theirs that the outcome is in $ {00, 0, 1, 2, 3, ... , 36 }$ (with probability 1) and be guaranteed a hefty profit. Unfortunately, we can&#39;t do that. We cannot even come close: in American roulette, the best possible bet is any one of the . The first step in any simulation is to write code that handles the logic of the generating process itself. As we describe above, roulette is a simple uniform random choice among $n$ pockets, where each of the possible bets has a payoff that is easily calculated from the uniform probability law (a table of these odds and expected values can be found at https://en.wikipedia.org/wiki/Roulette#Bet_odds_table). So one very generic way to describe roulette spins is through a function that takes the calculated win probability and returns True if a random float is within the win probability or False if the random float is outside the win probability. . def get_spin_result(win_prob): result = False if np.random.random() &lt;= win_prob: result = True return result . So, if we bet on Black (the closest thing to a fair bet we&#39;ll find in this game), we double our money if the ball lands on Black and lose it all if the ball lands on Red. (a 1:1 payout) . Doubling Down . Now we&#39;ve established what happens in a single round of roulette, we need to ask how to make generalizations about the game: what exactly can we control in this game? . Type of bets | Value of bets | For simplification sake, we&#39;re considering only one type of bet (though the . This is where the martingale strategy comes in: . After every win: Reset the wager $w$ to some base wager $n$ . After every loss: Double the wager to $2w$ . So after some number of losses (let&#39;s say $k$ losses), the next wager will be $n*2^{k}$ . Now we have a (not) new, enticingly simple strategy, the first order of business is to determine the performance of this strategy in our game. . With some sloppy hardcoding, our simulation code is the follwing: . def sim1k(win_prob, n=1000): # This will be a list of True/False boolean spin outcomes wl_seq = [get_spin_result(win_prob) for sim in range(n)] # Use these to collect our winnings, and set our win amount and bet size winnings = [] win = 0 bet = 1 # Progress through win loss outcomes in wl_seq for wl in wl_seq: # Cut off when total winnings reach $80 if win &lt; 80: # Resetting bet to 1 in case of win if wl == True: win = win + bet bet = 1 # Doubling in case of loss else: win = win - bet bet = bet * 2 winnings.append(win) return winnings . win_prob = (18./38.) tentrials = [sim1k(win_prob) for x in range(10)] for trial in tentrials: plt.plot(range(1000), trial) . ktrials = [sim1k(win_prob) for x in range(1000)] #print(sum(np.any(np.array(ktrials) == 80., axis=1)) / len(np.any(np.array(ktrials) == 80., axis=1))) ktmean = np.mean(ktrials, axis = 0) ktsd = np.std(ktrials, axis = 0) for arr in (ktmean, ktmean + ktsd, ktmean - ktsd): plt.plot(range(1000), arr) . ktmed = np.median(ktrials, axis = 0) for arr in (ktmed, ktmed + ktsd, ktmed - ktsd): plt.plot(range(1000), arr) . So what&#39;s the problem there? . Do not track bankroll - we can make huge losses during the interim . In any reasonable casino, there is a limit to bet size . Often the lower ceiling is the size of our own bank account . def sim1k_bankroll(win_prob, n=1000): wl_seq = [get_spin_result(win_prob) for sim in range(n)] winnings = [] win = 0 bet = 1 for wl in wl_seq: if (win &lt; 80 and win &gt; -256): if wl == True: win = win + bet bet = 1 else: win = win - bet bet = min(bet * 2, 256. + win) winnings.append(win) return winnings . ktrials_real = [sim1k_bankroll(win_prob) for x in range(1000)] ktmean = np.mean(ktrials_real, axis = 0) ktsd = np.std(ktrials_real, axis = 0) ktmed = np.median(ktrials_real, axis = 0) plt.figure() plt.xlim((0,300)) plt.ylim((-256,100)) for arr in (ktmean, ktmean + ktsd, ktmean - ktsd): plt.plot(range(1000), arr) . for arr in (ktmed, ktmed + ktsd, ktmed - ktsd): plt.plot(range(1000), arr) . What if I HAVE to play roulette? . First, my condolences to you and your wallet! . Fortunately, all is not lost. There is a strategy that can help you leave with your shirt. The intuition is explained in Berkolaiko&#39;s lecture: . If the game is subfair, every round we lose some (on average). . To minimize losses — minimize rounds! . Although we are in &quot;damage control&quot; mode, we are still being exposed to randomness that can cut both ways. In the long run, it will cut against us, but we can choose to simply not expose ourselves to the long run. We will bet aggressively and get out as soon as possible. . Conclusions . Sources . Betting systems:how not to lose your moneygambling (Gregory Berkolaiko) . Expected value and Betting systems (Luc Rey-Bellet) . Analysis of a Betting System (Rabung and Hyland) . A Statistical Analysis of the Roulette Martingale System: Examples, Formulas and Simulations with R (Peter Pflaumer) .",
            "url": "https://nshea3.github.io/blog/python/simulation/betting/strategy/martingale/2020/11/20/Simple-Betting-Strategies-Python.html",
            "relUrl": "/python/simulation/betting/strategy/martingale/2020/11/20/Simple-Betting-Strategies-Python.html",
            "date": " • Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "See nicholasshea.com [^1]. .",
          "url": "https://nshea3.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nshea3.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}