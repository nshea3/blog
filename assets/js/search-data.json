{
  
    
        "post0": {
            "title": "ConvNet House Numbers",
            "content": "Introduction . I recently encountered a problem at work where we could have made use of georeferenced, exterior images of single-family homes. . . Basically like this. . We immediately thought of Google Streetview and Zillow, but unfortunately both of those platforms have very restrictive use policies. We also considered using data from assessors&#39; offices, but that&#39;s difficult to do at scale because of the huge variety of ways that these images are stored, to say nothing of the metadata attached to them. . So we set out to find a single source of just images, no licensing or metadata to hold us back. Mapillary provides an open alternative of entirely crowdsourced photos shared from dashcams. Much better than the alternatives, but it leaves us with a different challenge: While we know what street we&#39;re on and roughly where we are, we don&#39;t know the camera parameters, so we can&#39;t precisely associate parts of the image with an address. We don&#39;t even have a particularly good idea of which direction the camera is pointed. . The natural solution to this problem is to do what humans did before GPS - look at road signs and house numbers! The address is literally painted all over the structure! We just have to segment it and extract it, ideally in an automated way. . It&#39;s difficult to imagine a script that we could explicitly program that would handle all the varying fonts, varying camera angles, varying lighting conditions, varying photo qualities that we&#39;ll see in reality. In short, this is a classic use case for deep learning, so we&#39;ll assemble and train a quick convolutional neural network here! . The dataset . For this demonstration we&#39;ll use the The Street View House Numbers (SVHN) Dataset from Stanford. This dataset contains a large number of labeled examples that we can use to train a convnet to label individual digits of a street number. . import scipy.io import tensorflow as tf from matplotlib.pyplot import * import numpy as np from tensorflow import keras from tensorflow.keras import layers from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() mat = scipy.io.loadmat(&#39;/content/drive/MyDrive/DATA/train_32x32.mat&#39;) mat_test = scipy.io.loadmat(&#39;/content/drive/MyDrive/DATA/test_32x32.mat&#39;) . This is what the 32x32 pixel digits look like: . ax = imshow(mat[&quot;X&quot;][:,:,:,3]) . ax = imshow(mat[&quot;X&quot;][:,:,:,115]) . And the associated labels: . mat[&quot;y&quot;][3] . array([3], dtype=uint8) . mat[&quot;y&quot;][115] . array([3], dtype=uint8) . Even this toy example includes some of the challenges we listed above (varying fonts, varying camera angles, varying lighting conditions, varying photo qualities) - and others that we didn&#39;t discuss: . Character level ground truth in an MNIST-like format. All digits have been resized to a fixed resolution of 32-by-32 pixels. The original character bounding boxes are extended in the appropriate dimension to become square windows, so that resizing them to 32-by-32 pixels does not introduce aspect ratio distortions. Nevertheless this preprocessing introduces some distracting digits to the sides of the digit of interest. . Preprocessing . We need to turn our array of RGB images into an array of greyscale images (it&#39;s definitely possible to do deep learning with RGB images, but it&#39;s a little more involved) And doing a little reshaping as well - need it in batch, pixel_x, pixel_y format. . We&#39;re using the . x_train = tf.transpose(np.tensordot(mat[&quot;X&quot;], [0.299, 0.587, 0.114], axes=([2], [0])), [2, 0, 1]) x_test = tf.transpose(np.tensordot(mat_test[&quot;X&quot;], [0.299, 0.587, 0.114], axes=([2], [0])), [2, 0, 1]) x_train.shape . TensorShape([73257, 32, 32]) . We&#39;re using the $v_{gray}(r,g,b) = 0.2989 * r + 0.5870 * g + 0.1140 * b$ formula to convert the (r,g,b) values into a grayscale value. . imshow(x_train[3,:,:]) . &lt;matplotlib.image.AxesImage at 0x7f441762d8d0&gt; . New! with zero color! . Now we move on to preparing the labels and further altering the shape of our tensor. . num_classes = len(np.unique(mat[&quot;y&quot;])) + 1 print(f&quot;Num classes: {num_classes}&quot;) y_train = mat[&quot;y&quot;].ravel() y_test = mat_test[&quot;y&quot;].ravel() input_shape = (32, 32, 1) def scale_and_expand(x_): return np.expand_dims((x_.astype(&quot;float32&quot;) / 255), -1) x_train, x_test = (scale_and_expand(x) for x in (x_train, x_test)) print(&quot;x_train shape:&quot;, x_train.shape) print(x_train.shape[0], &quot;train samples&quot;) print(x_test.shape[0], &quot;test samples&quot;) # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) . Num classes: 11 x_train shape: (73257, 32, 32, 1) 73257 train samples 26032 test samples . Model Topology . This is a simple ConvNet architecture (borrowed from here in the keras docs, credit to FranÃ§ois Chollet) . model = keras.Sequential( [ keras.Input(shape=input_shape), layers.Conv2D(32, kernel_size=(3, 3), activation=&quot;relu&quot;), layers.MaxPooling2D(pool_size=(2, 2)), layers.Conv2D(64, kernel_size=(3, 3), activation=&quot;relu&quot;), layers.MaxPooling2D(pool_size=(2, 2)), layers.Flatten(), layers.Dropout(0.5), layers.Dense(num_classes, activation=&quot;softmax&quot;), ] ) model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 30, 30, 32) 320 max_pooling2d (MaxPooling2D (None, 15, 15, 32) 0 ) conv2d_1 (Conv2D) (None, 13, 13, 64) 18496 max_pooling2d_1 (MaxPooling (None, 6, 6, 64) 0 2D) flatten (Flatten) (None, 2304) 0 dropout (Dropout) (None, 2304) 0 dense (Dense) (None, 11) 25355 ================================================================= Total params: 44,171 Trainable params: 44,171 Non-trainable params: 0 _________________________________________________________________ . Parameters and Training . batch_size = 128 epochs = 15 model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1) . Epoch 1/15 516/516 [==============================] - 101s 195ms/step - loss: 1.5204 - accuracy: 0.5037 - val_loss: 0.7716 - val_accuracy: 0.7935 Epoch 2/15 516/516 [==============================] - 87s 168ms/step - loss: 0.8102 - accuracy: 0.7637 - val_loss: 0.6294 - val_accuracy: 0.8354 Epoch 3/15 516/516 [==============================] - 77s 150ms/step - loss: 0.7143 - accuracy: 0.7977 - val_loss: 0.5900 - val_accuracy: 0.8460 Epoch 4/15 516/516 [==============================] - 89s 173ms/step - loss: 0.6613 - accuracy: 0.8141 - val_loss: 0.5400 - val_accuracy: 0.8598 Epoch 5/15 516/516 [==============================] - 88s 170ms/step - loss: 0.6289 - accuracy: 0.8232 - val_loss: 0.5257 - val_accuracy: 0.8597 Epoch 6/15 516/516 [==============================] - 83s 161ms/step - loss: 0.6051 - accuracy: 0.8284 - val_loss: 0.5008 - val_accuracy: 0.8677 Epoch 7/15 516/516 [==============================] - 80s 155ms/step - loss: 0.5827 - accuracy: 0.8354 - val_loss: 0.4865 - val_accuracy: 0.8657 Epoch 8/15 516/516 [==============================] - 79s 152ms/step - loss: 0.5680 - accuracy: 0.8377 - val_loss: 0.4763 - val_accuracy: 0.8647 Epoch 9/15 516/516 [==============================] - 79s 152ms/step - loss: 0.5498 - accuracy: 0.8421 - val_loss: 0.4652 - val_accuracy: 0.8743 Epoch 10/15 516/516 [==============================] - 78s 151ms/step - loss: 0.5381 - accuracy: 0.8475 - val_loss: 0.4508 - val_accuracy: 0.8721 Epoch 11/15 516/516 [==============================] - 82s 158ms/step - loss: 0.5284 - accuracy: 0.8489 - val_loss: 0.4342 - val_accuracy: 0.8793 Epoch 12/15 516/516 [==============================] - 78s 152ms/step - loss: 0.5147 - accuracy: 0.8515 - val_loss: 0.4243 - val_accuracy: 0.8804 Epoch 13/15 516/516 [==============================] - 78s 151ms/step - loss: 0.5066 - accuracy: 0.8536 - val_loss: 0.4201 - val_accuracy: 0.8795 Epoch 14/15 516/516 [==============================] - 78s 152ms/step - loss: 0.4945 - accuracy: 0.8564 - val_loss: 0.4294 - val_accuracy: 0.8855 Epoch 15/15 516/516 [==============================] - 78s 151ms/step - loss: 0.4897 - accuracy: 0.8574 - val_loss: 0.4050 - val_accuracy: 0.8833 . &lt;keras.callbacks.History at 0x7f4416c3fe10&gt; . Looking at the train/validation scores in the diagnostic output, I always have to remind myself that dropout means that validation accuracy will often be higher than training accuracy! Otherwise quite happy with the convergence of our model. Let&#39;s see how it does on a held-out test set: . score = model.evaluate(x_test, y_test, verbose=0) print(&quot;Test loss:&quot;, score[0]) print(&quot;Test accuracy:&quot;, score[1]) . Test loss: 0.4587405323982239 Test accuracy: 0.8741933107376099 . Not too bad for a first pass! . Sources: . Higher validation accuracy, than training accurracy using Tensorflow and Keras . | CS231n Convolutional Neural Networks for Visual Recognition: Lecture 3, Learning . | The Street View House Numbers (SVHN) Dataset . | Simple MNIST convnet . | .",
            "url": "https://nshea3.github.io/blog/keras/cnn/deeplearning/neuralnets/2022/11/05/_11_05_ConvNet_House_Numbers.html",
            "relUrl": "/keras/cnn/deeplearning/neuralnets/2022/11/05/_11_05_ConvNet_House_Numbers.html",
            "date": " â€¢ Nov 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Mining the Water Table",
            "content": "This is a competition from DrivenData that I&#39;ve wanted to enter for some time - it&#39;s a geospatial problem, and it&#39;s related to water, and it&#39;s related to development - this is a win-win-win from my perspective. . Of course, the first attempt is never actually going to be about winning, especially in an established competition like this one. Here I&#39;ll just try to figure out what data is available to us and what API the competition requires (what is the input to our pipeline? what does the output look like?) Sometimes I&#39;ll even use randomly generated mock predictions to validate the API - but in this case, I&#39;ll actually put together a simple baseline model. . First things first, what&#39;s the objective here? . Your goal is to predict the operating condition of a waterpoint for each record in the dataset. . So we have some working wells, and some broken wells, and it&#39;s our job to figure out which is which. . Next, some imports to make and some files to open: . !pip install geopandas &gt; /dev/null !pip install folium matplotlib mapclassify &gt; /dev/null !pip install contextily &gt; /dev/null . import pandas as pd import geopandas as gpd . train_labels = pd.read_csv(&quot;/content/drive/MyDrive/data_mining_water_table/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv&quot;) train_features = pd.read_csv(&quot;/content/drive/MyDrive/data_mining_water_table/4910797b-ee55-40a7-8668-10efd5c1b960.csv&quot;) test_features = pd.read_csv(&quot;/content/drive/MyDrive/data_mining_water_table/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv&quot;) . Labels . Looking at the label values: . train_labels.status_group.value_counts() . functional 32259 non functional 22824 functional needs repair 4317 Name: status_group, dtype: int64 . So this is a multi-class classification problem! I&#39;ll leave that be for the time being, but I think there may be a way around that particular hitch. . The class balance looks alright between functional and non functional, but functional needs repair is super underrepresented. . Only one more column to check here - it may say it&#39;s a primary key but I&#39;ve been burned too many times: . train_labels.id.is_unique . True . Features . At first glance I prefer to look at the .info() method output on the dataframe. I find it is a happy medium between looking at column names and diving right into the output of df.describe(). Datatypes and variable names tell us a lot about the features and what they could represent! . train_features.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 59400 entries, 0 to 59399 Data columns (total 40 columns): # Column Non-Null Count Dtype -- -- 0 id 59400 non-null int64 1 amount_tsh 59400 non-null float64 2 date_recorded 59400 non-null object 3 funder 55765 non-null object 4 gps_height 59400 non-null int64 5 installer 55745 non-null object 6 longitude 59400 non-null float64 7 latitude 59400 non-null float64 8 wpt_name 59400 non-null object 9 num_private 59400 non-null int64 10 basin 59400 non-null object 11 subvillage 59029 non-null object 12 region 59400 non-null object 13 region_code 59400 non-null int64 14 district_code 59400 non-null int64 15 lga 59400 non-null object 16 ward 59400 non-null object 17 population 59400 non-null int64 18 public_meeting 56066 non-null object 19 recorded_by 59400 non-null object 20 scheme_management 55523 non-null object 21 scheme_name 31234 non-null object 22 permit 56344 non-null object 23 construction_year 59400 non-null int64 24 extraction_type 59400 non-null object 25 extraction_type_group 59400 non-null object 26 extraction_type_class 59400 non-null object 27 management 59400 non-null object 28 management_group 59400 non-null object 29 payment 59400 non-null object 30 payment_type 59400 non-null object 31 water_quality 59400 non-null object 32 quality_group 59400 non-null object 33 quantity 59400 non-null object 34 quantity_group 59400 non-null object 35 source 59400 non-null object 36 source_type 59400 non-null object 37 source_class 59400 non-null object 38 waterpoint_type 59400 non-null object 39 waterpoint_type_group 59400 non-null object dtypes: float64(3), int64(7), object(30) memory usage: 18.1+ MB . We also get a preview of missingness. In a welcome departure from meticulously curated datasets that often show up in ML competitions, this dataset has missing data. This is a frequent problem in real ML problems and handling it gracefully is important in application. . What sticks out at me: . We have latitude and longitude here, so we are working with geospatial data as expected (yay!) | They are already floats, so we won&#39;t need to deal with strings or WKB or anything | None of them are missing! | . What will probably require more feature engineering work: . date_recorded - if we have datetimes, we will experience at least one datetime related error (sorry, that&#39;s just the rule!) And the year and/or month will probably be more useful than some high-cardinality year-month-day combination | . Speaking of cardinality - we will need to check on these object columns! We might need to process some of them to prune . And finally - some of these look like spatial subdivisions. Those require special care. I&#39;ll address that later. Fortunately, few of those seem to be missing. . Maps . So we&#39;ve confirmed that the coordinates exist - what&#39;s our next step? . What is absolutely positively the FIRST thing you should do with geospatial data? MAP IT! . The floating point latitude and longitude will take a little massaging to get into the right datatype for , but it&#39;s not anything geopandas can&#39;t handle: . train_feat_gdf = gpd.GeoDataFrame(train_features, geometry=gpd.points_from_xy( train_features.longitude, train_features.latitude) ).set_crs(epsg=4326) . This dataframe is big, so we should probably subsample first and restrict the values we display. . import contextily as cx ax = train_feat_gdf.merge(train_labels)[[&quot;geometry&quot;, &quot;status_group&quot;]].sample(frac=0.1).to_crs(epsg=3857).plot(figsize=(20, 12),column=&quot;status_group&quot;, legend=True) # ax = df_wm.plot(figsize=(10, 10), alpha=0.5, edgecolor=&#39;k&#39;) cx.add_basemap(ax) . The wells are pretty clustered, presumably around human settlements. The sparse areas are reserves and national parks, so we wouldn&#39;t expect a large or permanent human presence in that area. That can all be established from just the label locations, not their color. . From the class labels it is pretty clear that the non-functional wells are clustered in proximity to one another. This shouldn&#39;t be terribly surprising: we can imagine a number of different variables that vary spatially and affect the functioning of the wells (like the depth to water table, or geology, or topography, or a shared maintenance crew etc.) . This spatial correlation shows up in many different applications and there are many methods to help us make good spatial predictions. This will probably be important in this particular competition. . Given that we can see the spatial correlation visually, it makes sense to exploit it immediately. This can serve as a good baseline method. . Within the universe of spatial models there are still many to choose from. The simplest is probably a k nearest neighbors approach - where the class is determined by a vote of the nearest k labeled examples in feature space (in this case, $(x,y)$ space). . kNN is most appropriate for &quot;interpolation&quot; type problems, where the unlabeled data is interspersed with the labeled data and there are plenty of close neighbors to choose from. This is a common feature of spatial algorithms that use weighted combinations of surrounding values. I verified this earlier but didn&#39;t want to include the plot here to avoid bloating the notebook with maps. . So we can move right on to preprocessing, parameter selection, and training: . xy_ = train_features.merge(train_labels) from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score, cross_validate X = xy_[[&quot;longitude&quot;, &quot;latitude&quot;]] y = xy_[[&quot;status_group&quot;]] pipe = Pipeline( [ (&quot;s_scaler&quot;, StandardScaler()), (&quot;knn&quot;, KNeighborsClassifier()), ] ) . Explicitly setting the value of k to 5 neighbors: . pipe.set_params(knn__n_neighbors = 5) scores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[&quot;roc_auc_ovo&quot;, &quot;balanced_accuracy&quot;]) scores . {&#39;fit_time&#39;: array([0.1089015 , 0.08685112, 0.15483212, 0.22506523, 0.21545577]), &#39;score_time&#39;: array([0.60315633, 0.88687468, 1.12632608, 1.07287598, 0.4798944 ]), &#39;test_roc_auc_ovo&#39;: array([0.75283679, 0.74980639, 0.75823468, 0.74190514, 0.76256156]), &#39;test_balanced_accuracy&#39;: array([0.54690659, 0.53857098, 0.54173178, 0.54211186, 0.54501897])} . pipe.set_params(knn__n_neighbors = 7) scores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[&quot;roc_auc_ovo&quot;, &quot;balanced_accuracy&quot;]) scores . {&#39;fit_time&#39;: array([0.16143775, 0.08543348, 0.09065247, 0.0830009 , 0.08364654]), &#39;score_time&#39;: array([0.52614832, 0.47105384, 0.52411175, 0.51431751, 0.57632303]), &#39;test_roc_auc_ovo&#39;: array([0.76035781, 0.77105027, 0.76698738, 0.74877248, 0.77267746]), &#39;test_balanced_accuracy&#39;: array([0.52641583, 0.52729711, 0.52632049, 0.5237053 , 0.52658191])} . And now to 15: . pipe.set_params(knn__n_neighbors = 15) scores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[&quot;roc_auc_ovo&quot;, &quot;balanced_accuracy&quot;]) scores . {&#39;fit_time&#39;: array([0.08686829, 0.08921075, 0.08237839, 0.08246255, 0.15993857]), &#39;score_time&#39;: array([0.54348087, 0.54373407, 0.54726887, 0.95049524, 0.70698071]), &#39;test_roc_auc_ovo&#39;: array([0.77307768, 0.77400969, 0.76989494, 0.76127998, 0.77105859]), &#39;test_balanced_accuracy&#39;: array([0.51086299, 0.51277977, 0.5155871 , 0.51080323, 0.51121926])} . The ROC score improved! However the balanced accuracy did not. . kNN methods are known to struggle with imbalanced classes, and we are likely seeing the minority class get erased from the predictions as the neighborhood size increases and the local majority vote looks more and more like the global majority (which doesn&#39;t include many instances of the minority class, by definition) . The competition uses a global accuracy measurement that doesn&#39;t specifically penalize errors on the minority class, so we can simply optimize for AUC and improve our overall classification accuracy. . pipe.set_params(knn__n_neighbors = 20) scores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[&quot;roc_auc_ovo&quot;, &quot;balanced_accuracy&quot;]) scores . {&#39;fit_time&#39;: array([0.22514343, 0.08336806, 0.08348846, 0.08687043, 0.23844385]), &#39;score_time&#39;: array([0.80263448, 0.56800866, 0.54393411, 0.59026051, 1.0776782 ]), &#39;test_roc_auc_ovo&#39;: array([0.76983834, 0.7714384 , 0.76485649, 0.75795266, 0.76685553]), &#39;test_balanced_accuracy&#39;: array([0.49622907, 0.49721959, 0.50113215, 0.50283384, 0.49887269])} . pipe.set_params(knn__n_neighbors = 30) scores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[&quot;roc_auc_ovo&quot;, &quot;balanced_accuracy&quot;]) scores . {&#39;fit_time&#39;: array([0.0867455 , 0.08558059, 0.08764887, 0.09245777, 0.08821964]), &#39;score_time&#39;: array([0.60613155, 0.75048637, 0.71204829, 0.83812332, 0.60349774]), &#39;test_roc_auc_ovo&#39;: array([0.76066061, 0.76423555, 0.75789102, 0.75412041, 0.75974524]), &#39;test_balanced_accuracy&#39;: array([0.47806289, 0.48623149, 0.48762984, 0.48972768, 0.48137897])} . So based on these experiments (really, just a manual hyperparameter optimization routine) the optimal value for k is around 15. So we can train and predict using k=15: . pipe.set_params(knn__n_neighbors = 15) pipe = pipe.fit(X, y.values.ravel()) Xtest = test_features[[&quot;longitude&quot;, &quot;latitude&quot;]] test_features[&quot;status_group&quot;] = pipe.predict(Xtest) . test_features[[&quot;id&quot;, &quot;status_group&quot;]].to_csv( &quot;/content/drive/MyDrive/data_mining_water_table/nshea3_submission_110422_2.csv&quot;, index=False) . At the time, that puts me at number 4933 out of 5400 submissions with an accuracy of 0.6737 - which is calculated as $ frac{1}{N} sum_{1}^{N} I(y_i = hat{y}_i)$ . This result isn&#39;t super impressive but it shows that there is indeed spatial correlation in this problem that can be exploited. . Next steps . As I mentioned before: I don&#39;t like working with multiclass classification problems unless it is completely necessary. Multiclass problems and multiclass models and multiclass model diagnostics+interpretations are more difficult to understand and more difficult to explain. In this case, non functional and functional needs repair could be subsets of needs repair - so we can bundle them into that variable and perform a first stage of binary classification on needs repair versus not needing repair. Or we could bundle first the other way around - we&#39;ll try both and use whatever works best. . We&#39;ll address that in combination with the following: . Experiment with AutoML | Address the missing data problem | Rigorously model spatial dependence | .",
            "url": "https://nshea3.github.io/blog/2022/11/04/Data-Mining-the-Water-Table.html",
            "relUrl": "/2022/11/04/Data-Mining-the-Water-Table.html",
            "date": " â€¢ Nov 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Rotations",
            "content": "Rotations were a major stumbling block for me in my undergrad applied linear algebra class. I&#39;d taken a 300-level math major linear algebra course, and I figured a 400 level applied linear algebra course would be an easy follow-on to fulfill the upper division requirement for the math minor. . It was not. It was very hard. It included computational work and some fairly advanced applications. At the time, I did not know how to code so everything was pretty hopelessly abstract . Now, armed with 6 years of Python experience, I am working my way through Coding the Matrix, which I recommend very highly. It has helped me gain clarity on some of these challenging concepts. There&#39;s also a philosophical angle to it - throughout Dr. Klein articulates the advantages of mathematical learning through coding (emphasis mine): . Moreover, a linear-algebra instructor whose pupils are students of computer science has a special advantage:her students are computationally sophisticated. They have a learning modality that most students donâ€™tâ€”they can learn through reading, writing, debugging, and using computer programs.For example, there are several ways of writing a program for matrix-vector or matrix-matrix multiplication, each providing its own kernel of insight into the meaning of the operationâ€”and the experience of writing such programs is more effective in conveying this meaning and cementing the relationships between the operations than spending the equivalent time carrying out hand calculations. Computational sophistication also helps students in the more abstract, mathematical aspects of linear algebra. Acquaintance with object-oriented programming helps a student grasp the notion of a fieldâ€”a set of values together with operations on them. Acquaintance with subtyping prepares a student to understand that some vector spaces are inner product spaces. Familiarity with loops or recursion helps a student understand procedural proofs, e.g. of the existence of a basis or an orthogonal basis. Computational thinking is the term suggested by Jeannette Wing, former head of the National Science Foundationâ€™s directorate on Computer and Information Science and Engineering, to refer to the skills and concepts that a student of computer science can bring to bear. For this book, computational thinking is the road to mastering elementary linear algebra. . I am convinced that students of mathematics would be better served by a &quot;computational thinking&quot; approach to learning math - less memorization and plug-and-chug, pencil-and-paper scribbling and more reading and writing code - I know I certainly am! . The first time I covered this material I struggled very badly in many places. I couldn&#39;t grasp the connection between rotations, the complex plane, and roots of unity. That&#39;s simple enough in retrospect, but the material was overwhelming and the connection was totally lost on me at the time. In the defense of the instructors we were running behind, en route to the discrete Fourier transform and we didn&#39;t really have time to stop for straggler. . Now, with free time, a textbook, and some Python, I am free to build intuition and see these rotations for myself. . Parrots the right way up, all the way down . from PIL import Image import requests from io import BytesIO url = &quot;https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png&quot; from urllib.request import urlopen from PIL import Image img = Image.open(urlopen(url)) img . This is the best greyscale image I could find on short order (and it definitely helps to have a greyscale image because we are about to binarize it!) It&#39;s also convenient that it is an image with one and only one &quot;correct&quot; orientation - a right way up! . import numpy as np img_array_ = np.asarray(img) # binarize! img_array_bin_ = img_array_ &gt; 128 . import matplotlib.pyplot as plt # Unpacking the &quot;white&quot; points and scatterplotting plt.scatter(*img_array_bin_.nonzero(), s=0.1) plt.gcf().set_size_inches((4, 3)) . Converting the image to a numpy array did not go seamlessly. This is not the correct orientation. . Converting to complex coordinates is the first step towards rotating: . x,y = img_array_bin_.nonzero() coords_complex_ = x + y*1j . Our rotate method will return the rotated points . rotate_ = lambda x, theta: x*np.exp(1j * theta) . The rotation angle should be 180 degrees + 90 degrees to flip and then make vertical. This works out to $ frac{3 pi}{2}$ radians: . cc = rotate_(coords_complex_, 3*np.pi/2) plt.scatter(cc.real, cc.imag, s=0.1) plt.gcf().set_size_inches((3, 4)) . Changing the angle gets us a slightly different orientation: . cc = rotate_(coords_complex_, 8*np.pi/5) plt.scatter(cc.real, cc.imag, s=0.1) plt.gcf().set_size_inches((3, 4)) . It&#39;s a trivial application but it helped me understand the concepts at work here. . As Dr. Klein said: . they can learn through reading, writing, debugging, and using computer programs. . Yes we can! .",
            "url": "https://nshea3.github.io/blog/2022/04/03/Rotations.html",
            "relUrl": "/2022/04/03/Rotations.html",
            "date": " â€¢ Apr 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Tedium Free MLE",
            "content": "Introduction . Maximum likelihood estimation has the dubious honor of being difficult for humans and machines alike (difficult for machines at least in the naÃ¯ve formulation that doesn&#39;t use log-likelihood). . MLE is challenging for humans because it requires the multiplication of $n$ likelihood expressions, which is time consuming and error prone - this is the tedium part we&#39;re trying to avoid. Fortunately, computers are very good at repeated multiplication, even repeated symbolic multiplication. . Problem Formulation and Example . MLE estimates parameters of an assumed probability distribution, given data $x_i$ observed independently from the same distribution. If that distribution has probability function $f( cdot)$, then the likelihood of $x_i$ is $f(x_i)$. . As the $x_i$s are independent, the likelihood of all $x_i$s will be the product of their individual likelihoods. In mathematical notation, the product will be: . $$ prod_{i=1}^{n} f(x_i)$$ . Probability functions (mass functions or density functions) like our $f( cdot)$ typically have parameters. For instance, the Gaussian distribution has parameters $ mu$ and $ sigma^2$, and the Poisson distribution has rate parameter Î». We use MLE to estimate these parameters, so they are the unknowns in the expression and they will appear in each $f(x_i)$ term. We can restate the problem as an equality with the generic parameter $ theta$: . $$L( theta) = prod_{i=1}^{n} f(x_i)$$ . The expression $L( theta)$ is the likelihood. In order to find the MLE it is necessary to maximize this function, or find the value of $ theta$ for which $L( theta)$ is as large as possible. This process is probably easier to show than to describe. In particular, we&#39;ll be demonstrating the usefulness of the sympy module in making these symbolic calculations. . Example . Say we observed values $[3,1,2]$ generated from a Poisson. What is likelihood function of Î»? . Importing the necessities and setting up some symbols and expressions: . from sympy.stats import Poisson, density, E, variance from sympy import Symbol, simplify from sympy.abc import x lambda_ = Symbol(&quot;lambda&quot;, positive=True) f = Poisson(&quot;f&quot;, lambda_) density(f)(x) . $ displaystyle frac{ lambda^{x} e^{- lambda}}{x!}$ sympy gives us a representation of the Poisson density to work with in the Poisson() object, keeping track of all of the terms internally. . The likelihood expression is the product of the probability function evaluated at these three points: . L_ = density(f)(3) * density(f)(1) * density(f)(2) L_ . $ displaystyle frac{ lambda^{6} e^{- 3 lambda}}{12}$ That&#39;s our expression for the likelihood $L( theta)$ ðŸ™‚ In order to maximize the expression, we&#39;ll take the derivative expression and then solve for the value of parameter $ lambda$ where the derivative expression is equal to 0. This value of $ lambda$ will maximize the likelihood. . Finding the derivative using sympy: . from sympy import diff dL_ = diff(L_, lambda_) dL_ . $ displaystyle - frac{ lambda^{6} e^{- 3 lambda}}{4} + frac{ lambda^{5} e^{- 3 lambda}}{2}$ Setting the derivative $ frac{dL}{d theta}$ equal to zero: . from sympy import Eq dLeqz = Eq(dL_, 0) dLeqz . $ displaystyle - frac{ lambda^{6} e^{- 3 lambda}}{4} + frac{ lambda^{5} e^{- 3 lambda}}{2} = 0$ And finally, solving the equation for $ lambda$: . from sympy import solve solve(dLeqz, lambda_) . [2] . And that&#39;s our answer! . Complications . There is a slight wrinkle with this approach. It is susceptible to numerical instability, which (luckily) did not affect us in this example. This is how MLE can become difficult for computers too. . Likelihoods are usually very small numbers and computers simply can&#39;t track numbers that are too small or too large. Multiplying very small numbers together repeatedly makes very VERY small numbers that can sometimes disappear completely. Without getting too distracted by the minutiae of numerical stability or underflow, we can still appreciate some bizarre behavior that results when floats are misused: . 6.89 + .1 . 6.989999999999999 . (0.1)**512 . 0.0 . In the second scenario, we can imagine having 512 data points and finding that the likelihood evaluates to 0.1 (times our parameter) for every single one. Then our product would look like $g( theta) cdot (0.1)^{512}$. The computer just told us that one of those terms is zero, and we&#39;re left unable to find the parameters for our MLE. . Solution . What do we do instead? Is there any way to make these numbers bigger, without changing the problem or solution? Is there an equivalent problem with bigger numbers? . Adding a number and multiplying by a number don&#39;t fix the problem - they just add terms to the expression, which ends up zero anyhow. However these functions do have one property that we will need to be sure we are solving an equivalent problem: they preserve the order of the input in the output. We call these functions monotonic. . The monotonic functions also include the log function. The log function has some very nice properties, not least of which that it makes our calculations immune to the problems we saw above. Calculating the log likelihood: . from sympy import log _ = simplify(log(L_)) _ . $ displaystyle - 3 lambda + 6 log{ left( lambda right)} - log{ left(12 right)}$ And then taking the derivative as before: . d_ = diff(_, lambda_) d_ . Setting equal to zero: . _ = Eq(_, 0) _ . $ displaystyle -3 + frac{6}{ lambda} = 0$ And solving: . from sympy import solve solve(_, lambda_) . [2] . The two solutions agree! Which is necessary, but not sufficient to show these methods are equivalent in general. .",
            "url": "https://nshea3.github.io/blog/2021/10/19/Tedium-Free-MLE.html",
            "relUrl": "/2021/10/19/Tedium-Free-MLE.html",
            "date": " â€¢ Oct 19, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "The birthday problem here and elsewhere",
            "content": "Introduction . The birthday problem is a classic probabilistic puzzle concerning the probability of any two people among $n$ people in a group sharing the same birthday. . In a group of $n$ people, what is the probability that two of these people share a birthday? . The intuition around the puzzle is very misleading, so much so that it is sometimes called the birthday paradox, even though there is ultimately no paradox in the problem or its solution. . Birthday Problem Here . In fact, given some reasonable assumptions, there is a simple closed form expression for the probability:$p(n)= frac{365 !}{365^{n}(365-n) !}$ . under the assumption that all $n$ birthdays are drawn uniformly at random from 365 possible dates. . However, this model leads to some very surprising results when we consider the probabilities of finding shared birthdays in groups of different size $n$. . We can ask more interesting questions: for instance, what group size $n$ would result in a 50% chance of a shared birthday within the group? As an upper bound on $n$, a party of size $n=366$ must have two individuals that share the same birthday, for $p=1.0$, since there are only 365 unique birthdays to be assigned (by the famous pigeonhole principle). The answer for $p=0.5$, it turns out, is only $n=23$. . There is &gt;50% probability that among a group of 23 people, 2 share a birthday! . This is such a surprising conclusion that the whole problem is sometimes called a paradox! . Fortunately, verifying this result can be done with simulation just as well as with analytical methods. The code below assembles groups individual by individual, stopping when two individuals share a birthday. This is detected when a list of day numbers differs in size from a set of day numbers, because a set discards duplicates while a list does not: . import random def birthday_sim(n_days=365): dates = [] while len(set(dates)) == len(dates): dates.append(random.randint(1,n_days + 1)) return len(dates) . This code can easily run 100,000 simulations: . sims_100k = [birthday_sim() for x in range(100000)] . The distribution is right-skewed, which we&#39;d expect with a distribution centered at near 20 but taking values up to 365. The peak near 20 confirms the conclusion above that a value in this area corresponds to the 50th percentile, or the approximation of $p=0.5$ in this sample. . import matplotlib.pyplot as plt hist100k = plt.hist(sims_100k, bins = 89) . The median represents the 50th percentile/2nd quantile/50% probability and sits exactly at 23: . import statistics statistics.median(sims_100k) . 23.0 . The CDF gives a nice view of the problem: now we can read off the probabilities and the corresponding group size $n$. Here we find a few more surprising conclusions: we hit 80% probability of having a shared birthday just under $n=40$, and a 90% probability just over $n=40$. . hist100k = plt.hist(sims_100k, bins = 89, cumulative=True, density=True) . In group sizes over $n=60$ finding a shared birthday should be a near certainty. The table puts the probability at 99.4% where $n=60$. . This phenomenon is likely so unintuitive partially because the question is misunderstood. &quot;Find any two people with the same birthday&quot; can so easily be misunderstood as &quot;Select one person, and find another person with the same birthday&quot;, or &quot;find one person in the group that shares MY birthday&quot;. Neither of these is the correct interpretation. There is no need to match a specific person, or a specific date. These interpretations lack the combinatorial characteristic that makes these probabilities so high for low $n$; there are just so many different ways of combining $n$ people into groups of two! . Birthday Problem Elsewhere . Another advantage of the simulation method is flexibility. If we imagine we&#39;re meeting groups of $n$ individuals born on a different planet with a different year length, there is no need to re-derive the equation above because the simulation method adapts seamlessly to the new problem. We simply have to change the n_days parameter. . For example, a single year lasts 88 earth days on Mercury (though a Mercury day lasts 2 Mercury years!): . mercury_sims_100k = [birthday_sim(88) for x in range(100000)] hist100k = plt.hist(mercury_sims_100k, bins = 39, cumulative=True, density=True) . statistics.median(mercury_sims_100k) . 12.0 . So the corresponding value for 50% probability on Mercury? $n=12$. . Clearly, the simulation technique is well suited to this problem - in fact, this class of problems - and might even be considered an improvement on the available analytical methods! .",
            "url": "https://nshea3.github.io/blog/2021/01/19/Birthday-problem-here-and-elsewhere.html",
            "relUrl": "/2021/01/19/Birthday-problem-here-and-elsewhere.html",
            "date": " â€¢ Jan 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "The Mathematics of Drilling Intercepts",
            "content": "Introduction . Drilling intercepts are a prominent feature of junior mining news releases. They are closely monitored by the mining investment community, and a particularly good intercept can raise the prospects for a project. . As an example, consider this November 10 2020 release from Freegold Ventures: . Freegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit . The market responded with a 3% boost in the share price the next trading day, so clearly this was regarded as a positive signal for the company&#39;s prospects. (This is typical:capital markets tend to treat any news of this sort as good news.) The implications for the economic, geological, and engineering variables surrounding the project are much less clear. Is this a good geological result? Is it a good engineering result? Intercepts are highlights: incomplete data, collected and released selectively, so is it even possible to make an informed judgement using these numbers? . To complicate things even further, the selectively reported drilling intercepts are usually released in a rather complex manner, which can make it difficult to distinguish between truly good numbers and deceptively good results. Drilling intercepts are discussed at great length in other sources (here and here and here) but we&#39;ll take a mathematical perspective and develop a model that describes nested intercept configurations of arbitrary complexity. . We&#39;ll take Great Bear Resources for an extended example. Great Bear Resources is a Canadian junior mining company whose stock gained substantially on announcement of very high grade intercepts at their Dixie project in Ontario. At time of writing, GBR is trading at a $886 million CAD market cap (which is not very bearish at all!) . GBR Price Today by TradingView Here we open up the spreadsheet of drilling results (available on their website), and then filter on Drill Hole to consider a single hole: . import pandas as pd intercepts = pd.read_excel(&#39;drive/My Drive/Projects/posts/data/Great_Bear/lp_drill_hole_composites_all.xlsx&#39;) intercepts[&#39;Record&#39;] = intercepts.index dh = intercepts[intercepts[&#39;Drill Hole&#39;] == &#39;BR-022&#39;] dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | . This is how intercepts are typically presented: a table with a From field describing where they started measuring, a To field describing where they stopped, and a Grade field (called Gold here) that tells us how enriched that interval is with the valuable stuff. From and To are typically measured downhole from the drill collar. . It&#39;s easy to establish a basic understanding of how these tables are read, and many experienced mining investors immediately recognize these grades as very high. The rest of us might need to rely on statistics, since we don&#39;t have the benefit of many years&#39; experience with drilling results. . Of course it is first necessary to determine the true assay values for each separate interval from top to bottom. Unfortunately, each row is not independent - some of the intercepts are contained in others, and the subinterval gold is INCLUDED in the parent interval calculation! So we can&#39;t just use the Gold (g/t) field directly, since intercepts are reported with these &quot;highlights&quot;, or higher grade sections within the longer interval. . Sometimes this convention is used unethically to suggest larger intervals of enrichment than truly exist. This is called &quot;grade smearing&quot; and the method of residual grade calculation applied here will detect any such attempt to disguise poor results. . At first it may seem like the correct interpretation of these intervals is to imagine them intervals stacked on top of one another, but this is very misleading. We can easily visualize this to see the error: . We only have the total grade, INCLUDING the high-grade, child subintervals. Considering it in that way ignores the fact that the high-grade intervals are included in the wider, lower-grade intervals, inflating the grade measured over that length. This has enormous implications for the continuity of the mineralization, which determines the feasibility of the project. | . In order to eliminate this effect we&#39;ll need to do some math with the intercepts. This visualization attempts to show this hierarchical, branching structure: . Plotted side by side, the intercepts show the parent-child overlapping relationship and capture the complexity of the problem. . Parent intervals can have no child intervals, a single child interval, or several child intervals. Child intervals themselves can have no child intervals, a single child interval, or several child intervals. Clearly there is a whole class of related problems we could solve with a general solution to this problem. . So far we have treated the From and To fields in isolation, and we can use a cool feature of Pandas to convert them to intervals: . dh[&#39;Interval&#39;] = dh.apply(lambda x: pd.Interval(x[&#39;From (m)&#39;], x[&#39;To (m)&#39;]), axis=1) dh . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . So the motivation here was to create Interval objects to use them with the pd.Overlaps function and then model the overlap relationship among the different intervals: . import itertools import numpy as np cross_interval = itertools.product(dh.Interval,dh.Interval) overlap_matrix = np.array([interval[0].overlaps(interval[1]) for interval in cross_interval]) intersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval]) ns = int(np.sqrt(overlap_matrix.shape[0])) . Here we see the overlaps: if a pixel is white, it means that the interval on the x-axis and the interval on the y-axis overlap. . Overlap is symmetric: so each &#39;child&#39; overlaps with its parent and vice versa. It should become clear that we are actually interested in the &quot;contains&quot; relationship, which is not symmetric and will help us identify parent intervals and child intervals and start reducing the intervals. . Fortunately this is also supported in Python: . from sympy import Interval dh[&#39;Interval_obj&#39;] = dh.apply(lambda x: Interval(x[&#39;From (m)&#39;], x[&#39;To (m)&#39;]), axis=1) cross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj) contain_matrix = np.array([interval[0].is_proper_superset(interval[1]) for interval in cross_interval]) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Now we can pull out a tree . Of the machine-intelligible formats, a tree data structure is clearly the most suited to representing the intervals. . for i, col in enumerate(contain_matrix_sq.T): if ~np.any(col): all_str_to_node[str(dh[&#39;Record&#39;].values[i])].parent = root else: all_str_to_node[str(dh[&#39;Record&#39;].values[i])].parent = all_str_to_node[str(dh[&#39;Record&#39;].values[::-1][np.argmax(col[::-1])])] print(RenderTree(root, style=AsciiStyle()).by_attr()) . DH |-- 95 | +-- 96 |-- 97 |-- 98 | +-- 99 |-- 100 |-- 101 | +-- 102 | |-- 103 | |-- 104 | +-- 105 | +-- 106 +-- 107 . Now we are really getting somewhere- we can actually start looking at the global picture (since we now know which intervals are not &quot;child&quot; intervals) . These are the direct children. We can go ahead and plot them and have a totally accurate picture of the log: . dh_prime.dtypes . Drill Hole object Unnamed: 1 object From (m) float64 To (m) float64 Width (m) float64 Gold (g/t) float64 Record int64 Interval interval[float64] Interval_obj object dtype: object . While that is correct, it is not complete: we have left out all of the additional information provided by the smaller sub-intervals! . In order to incorporate that we will have to remove them from the parent intervals and determine the residual grade (whatever is left once we pull out the gold contained in the subinterval) . ((119) * (3.78) - (3) * (131.5)) / (119 - 3) . 0.47689655172413786 . As an example of this kind of calculation, a simpler set of intervals from a Freegold Ventures press release: . Freegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit . We know the gold grade over the whole 119 meters, and the gold grade over 3 meters, but what is the gold grade over the $119 - 3 = 116 m$? . The solution is a simple weighted average calculation, like compositing over a drillhole:$ frac{119 times 3.78-3 times 131.5}{119-3} = 0.477 g/t$ . Credit to https://twitter.com/BrentCo77759016/status/1326183861722599424 and . So now we have to do this, but with every single subinterval until we get the residual grade at every point along the drillhole . Fortunately, the tree data structure we selected has specialized methods that make a traversal very simple. . levelord_nodes = [(node.name, node.children) for node in LevelOrderIter(root)] levelord_nodes . [(&#39;DH&#39;, (Node(&#39;/DH/95&#39;), Node(&#39;/DH/97&#39;), Node(&#39;/DH/98&#39;), Node(&#39;/DH/100&#39;), Node(&#39;/DH/101&#39;), Node(&#39;/DH/107&#39;))), (&#39;95&#39;, (Node(&#39;/DH/95/96&#39;),)), (&#39;97&#39;, ()), (&#39;98&#39;, (Node(&#39;/DH/98/99&#39;),)), (&#39;100&#39;, ()), (&#39;101&#39;, (Node(&#39;/DH/101/102&#39;),)), (&#39;107&#39;, ()), (&#39;96&#39;, ()), (&#39;99&#39;, ()), (&#39;102&#39;, (Node(&#39;/DH/101/102/103&#39;), Node(&#39;/DH/101/102/104&#39;), Node(&#39;/DH/101/102/105&#39;))), (&#39;103&#39;, ()), (&#39;104&#39;, ()), (&#39;105&#39;, (Node(&#39;/DH/101/102/105/106&#39;),)), (&#39;106&#39;, ())] . nn_np_loi = [(node.name, node.parent) for node in LevelOrderIter(root)] . all_str_to_node . {&#39;100&#39;: Node(&#39;/DH/100&#39;), &#39;101&#39;: Node(&#39;/DH/101&#39;), &#39;102&#39;: Node(&#39;/DH/101/102&#39;), &#39;103&#39;: Node(&#39;/DH/101/102/103&#39;), &#39;104&#39;: Node(&#39;/DH/101/102/104&#39;), &#39;105&#39;: Node(&#39;/DH/101/102/105&#39;), &#39;106&#39;: Node(&#39;/DH/101/102/105/106&#39;), &#39;107&#39;: Node(&#39;/DH/107&#39;), &#39;95&#39;: Node(&#39;/DH/95&#39;), &#39;96&#39;: Node(&#39;/DH/95/96&#39;), &#39;97&#39;: Node(&#39;/DH/97&#39;), &#39;98&#39;: Node(&#39;/DH/98&#39;), &#39;99&#39;: Node(&#39;/DH/98/99&#39;)} . for node, parent in nn_np_loi[::-1][:-1]: print(node) for child in all_str_to_node[node].children: print(child) . 106 105 Node(&#39;/DH/101/102/105/106&#39;) 104 103 102 Node(&#39;/DH/101/102/103&#39;) Node(&#39;/DH/101/102/104&#39;) Node(&#39;/DH/101/102/105&#39;) 99 96 107 101 Node(&#39;/DH/101/102&#39;) 100 98 Node(&#39;/DH/98/99&#39;) 97 95 Node(&#39;/DH/95/96&#39;) . dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record Interval Interval_obj . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | (110.0, 116.1] | Interval(110.000000000000, 116.100000000000) | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | (111.4, 113.1] | Interval(111.400000000000, 113.100000000000) | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | (274.0, 299.0] | Interval(274.000000000000, 299.000000000000) | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | (432.9, 439.0] | Interval(432.900000000000, 439.000000000000) | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | (432.9, 435.7] | Interval(432.900000000000, 435.700000000000) | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | (445.0, 452.0] | Interval(445.000000000000, 452.000000000000) | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | (461.6, 512.0] | Interval(461.600000000000, 512.000000000000) | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | (471.0, 512.0] | Interval(471.000000000000, 512.000000000000) | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | (471.0, 478.0] | Interval(471.000000000000, 478.000000000000) | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | (490.0, 491.0] | Interval(490.000000000000, 491.000000000000) | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | (505.2, 508.75] | Interval(505.200000000000, 508.750000000000) | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | (506.2, 506.7] | Interval(506.200000000000, 506.700000000000) | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | (600.0, 620.0] | Interval(600.000000000000, 620.000000000000) | . cross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj) intersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval]) intersect_matrix . array([Interval(110.000000000000, 116.100000000000), Interval(111.400000000000, 113.100000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(111.400000000000, 113.100000000000), Interval(111.400000000000, 113.100000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(274.000000000000, 299.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(432.900000000000, 439.000000000000), Interval(432.900000000000, 435.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(432.900000000000, 435.700000000000), Interval(432.900000000000, 435.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(445.000000000000, 452.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(461.600000000000, 512.000000000000), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 478.000000000000), Interval(490.000000000000, 491.000000000000), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 512.000000000000), Interval(471.000000000000, 478.000000000000), Interval(490.000000000000, 491.000000000000), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(471.000000000000, 478.000000000000), Interval(471.000000000000, 478.000000000000), Interval(471.000000000000, 478.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(490.000000000000, 491.000000000000), Interval(490.000000000000, 491.000000000000), EmptySet(), Interval(490.000000000000, 491.000000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(505.200000000000, 508.750000000000), Interval(505.200000000000, 508.750000000000), EmptySet(), EmptySet(), Interval(505.200000000000, 508.750000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(506.200000000000, 506.700000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), Interval(506.200000000000, 506.700000000000), Interval(506.200000000000, 506.700000000000), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(), Interval(600.000000000000, 620.000000000000)], dtype=object) . dh[&#39;grade_len&#39;] = dh[&#39;Gold (g/t)&#39;] * dh[&#39;Width (m)&#39;] . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . dh . Drill Hole Unnamed: 1 From (m) To (m) Width (m) Gold (g/t) Record Interval Interval_obj grade_len . 95 BR-022 | including | 110.0 | 116.10 | 6.10 | 2.62 | 95 | (110.0, 116.1] | Interval(110.000000000000, 116.100000000000) | 15.982 | . 96 BR-022 | and including | 111.4 | 113.10 | 1.70 | 7.92 | 96 | (111.4, 113.1] | Interval(111.400000000000, 113.100000000000) | 13.464 | . 97 BR-022 | and | 274.0 | 299.00 | 25.00 | 0.19 | 97 | (274.0, 299.0] | Interval(274.000000000000, 299.000000000000) | 4.750 | . 98 BR-022 | and | 432.9 | 439.00 | 6.10 | 4.05 | 98 | (432.9, 439.0] | Interval(432.900000000000, 439.000000000000) | 24.705 | . 99 BR-022 | including | 432.9 | 435.70 | 2.80 | 8.18 | 99 | (432.9, 435.7] | Interval(432.900000000000, 435.700000000000) | 22.904 | . 100 BR-022 | and | 445.0 | 452.00 | 7.00 | 0.41 | 100 | (445.0, 452.0] | Interval(445.000000000000, 452.000000000000) | 2.870 | . 101 BR-022 | and | 461.6 | 512.00 | 50.40 | 1.78 | 101 | (461.6, 512.0] | Interval(461.600000000000, 512.000000000000) | 89.712 | . 102 BR-022 | including | 471.0 | 512.00 | 41.00 | 2.09 | 102 | (471.0, 512.0] | Interval(471.000000000000, 512.000000000000) | 85.690 | . 103 BR-022 | and including | 471.0 | 478.00 | 7.00 | 2.37 | 103 | (471.0, 478.0] | Interval(471.000000000000, 478.000000000000) | 16.590 | . 104 BR-022 | and including | 490.0 | 491.00 | 1.00 | 8.15 | 104 | (490.0, 491.0] | Interval(490.000000000000, 491.000000000000) | 8.150 | . 105 BR-022 | and including | 505.2 | 508.75 | 3.55 | 14.90 | 105 | (505.2, 508.75] | Interval(505.200000000000, 508.750000000000) | 52.895 | . 106 BR-022 | and including | 506.2 | 506.70 | 0.50 | 100.48 | 106 | (506.2, 506.7] | Interval(506.200000000000, 506.700000000000) | 50.240 | . 107 BR-022 | and | 600.0 | 620.00 | 20.00 | 0.52 | 107 | (600.0, 620.0] | Interval(600.000000000000, 620.000000000000) | 10.400 | . from sympy import Union import functools resid_grades, resid_len = {}, {} for node in levelord_nodes[1:]: parent_interval = dh[dh[&#39;Record&#39;] == float(node[0])] child_names = [child.name for child in node[1]] child_intervals = [dh[dh[&#39;Record&#39;] == float(child)] for child in child_names] new_interval_obj = parent_interval.Interval_obj.values[0] - Union([child.Interval_obj.values[0] for child in child_intervals]) l_child_int = Union([intv[&#39;Interval_obj&#39;].values[0] for intv in child_intervals])._measure lg_child_int = [dh.loc[int(child_name)][&#39;grade_len&#39;] for child_name in child_names] lg_total_int = parent_interval.grade_len.values[0] residual_grade = (lg_total_int - sum(lg_child_int)) / (new_interval_obj._measure) resid_grades[node[0]] = residual_grade resid_len[node[0]] = new_interval_obj print(&quot;Interval:&quot;) print(node[0]) print(&quot;Length x Grade:&quot;) print(lg_total_int - sum(lg_child_int)) print(&quot;Residual Grade:&quot;) print(residual_grade) . Interval: 95 Length x Grade: 2.517999999999999 Residual Grade: 0.572272727272726 Interval: 97 Length x Grade: 4.75 Residual Grade: 0.190000000000000 Interval: 98 Length x Grade: 1.801000000000002 Residual Grade: 0.545757575757574 Interval: 100 Length x Grade: 2.8699999999999997 Residual Grade: 0.410000000000000 Interval: 101 Length x Grade: 4.022000000000006 Residual Grade: 0.427872340425534 Interval: 107 Length x Grade: 10.4 Residual Grade: 0.520000000000000 Interval: 96 Length x Grade: 13.464 Residual Grade: 7.92000000000005 Interval: 99 Length x Grade: 22.903999999999996 Residual Grade: 8.17999999999997 Interval: 102 Length x Grade: 8.055000000000007 Residual Grade: 0.273514431239389 Interval: 103 Length x Grade: 16.59 Residual Grade: 2.37000000000000 Interval: 104 Length x Grade: 8.15 Residual Grade: 8.15000000000000 Interval: 105 Length x Grade: 2.654999999999994 Residual Grade: 0.870491803278683 Interval: 106 Length x Grade: 50.24 Residual Grade: 100.480000000000 . Check these solutions: 95 should be easy to validate . includes 96 . 95 extends from 110.00 m to 116.10 m (l = 6.10 m) . 96 is the interval from 111.40 to 113.10 m (l = 1.70 m) . Larger interval . (6.10 m)(2.62 g/t) = (1.7 m)(7.92 g/t) + (4.4 m)(x g/t) . Rearranging terms, we get: . $x = frac{(6.10 m)(2.62 g/t) - (1.7 m)(7.92 g/t)}{ 4.4 m }$ . So the residual grade is 0.5723 g/t, which matches the value found above! . ((6.10 * 2.62) - (1.7)*(7.92)) / (4.4) . 0.5722727272727269 . resid_len . {&#39;100&#39;: Interval(445.000000000000, 452.000000000000), &#39;101&#39;: Interval.Ropen(461.600000000000, 471.000000000000), &#39;102&#39;: Union(Interval.open(478.000000000000, 490.000000000000), Interval.open(491.000000000000, 505.200000000000), Interval.Lopen(508.750000000000, 512.000000000000)), &#39;103&#39;: Interval(471.000000000000, 478.000000000000), &#39;104&#39;: Interval(490.000000000000, 491.000000000000), &#39;105&#39;: Union(Interval.Ropen(505.200000000000, 506.200000000000), Interval.Lopen(506.700000000000, 508.750000000000)), &#39;106&#39;: Interval(506.200000000000, 506.700000000000), &#39;107&#39;: Interval(600.000000000000, 620.000000000000), &#39;95&#39;: Union(Interval.Ropen(110.000000000000, 111.400000000000), Interval.Lopen(113.100000000000, 116.100000000000)), &#39;96&#39;: Interval(111.400000000000, 113.100000000000), &#39;97&#39;: Interval(274.000000000000, 299.000000000000), &#39;98&#39;: Interval.Lopen(435.700000000000, 439.000000000000), &#39;99&#39;: Interval(432.900000000000, 435.700000000000)} . Interval(445.000000000000, 452.000000000000).end . 452.000000000000 . dh[&#39;Record&#39;].astype(str).map(resid_len) dh[&#39;Record&#39;].astype(str).map(resid_grades) . 95 0.572272727272726 96 7.92000000000005 97 0.190000000000000 98 0.545757575757574 99 8.17999999999997 100 0.410000000000000 101 0.427872340425534 102 0.273514431239389 103 2.37000000000000 104 8.15000000000000 105 0.870491803278683 106 100.480000000000 107 0.520000000000000 Name: Record, dtype: object . TODO: Need to split up the non-contiguous segments so that you can actually plot them . Idea: use .args attribute Problem: This is defined for Interval objects as well and it gets us something we don&#39;t want . dh[&#39;Record&#39;].astype(str).map(resid_len) . 95 Union(Interval.Ropen(110.000000000000, 111.400... 96 Interval(111.400000000000, 113.100000000000) 97 Interval(274.000000000000, 299.000000000000) 98 Interval.Lopen(435.700000000000, 439.000000000... 99 Interval(432.900000000000, 435.700000000000) 100 Interval(445.000000000000, 452.000000000000) 101 Interval.Ropen(461.600000000000, 471.000000000... 102 Union(Interval.open(478.000000000000, 490.0000... 103 Interval(471.000000000000, 478.000000000000) 104 Interval(490.000000000000, 491.000000000000) 105 Union(Interval.Ropen(505.200000000000, 506.200... 106 Interval(506.200000000000, 506.700000000000) 107 Interval(600.000000000000, 620.000000000000) Name: Record, dtype: object . def args_extended(interval): if type(interval) == Union: return interval.args else: return interval remaining_interval_grade = pd.DataFrame({&#39;interval&#39; : dh[&#39;Record&#39;].astype(str).map(resid_len), &#39;grade&#39; : dh[&#39;Record&#39;].astype(str).map(resid_grades)}) remaining_interval_grade[&#39;split_intervals&#39;] = remaining_interval_grade.interval.apply(args_extended) rig_exploded = remaining_interval_grade.explode(&#39;split_intervals&#39;) . rig_exploded . interval grade split_intervals . 95 Union(Interval.Ropen(110.000000000000, 111.400... | 0.572272727272726 | Interval.Ropen(110.000000000000, 111.400000000... | . 95 Union(Interval.Ropen(110.000000000000, 111.400... | 0.572272727272726 | Interval.Lopen(113.100000000000, 116.100000000... | . 96 Interval(111.400000000000, 113.100000000000) | 7.92000000000005 | Interval(111.400000000000, 113.100000000000) | . 97 Interval(274.000000000000, 299.000000000000) | 0.190000000000000 | Interval(274.000000000000, 299.000000000000) | . 98 Interval.Lopen(435.700000000000, 439.000000000... | 0.545757575757574 | Interval.Lopen(435.700000000000, 439.000000000... | . 99 Interval(432.900000000000, 435.700000000000) | 8.17999999999997 | Interval(432.900000000000, 435.700000000000) | . 100 Interval(445.000000000000, 452.000000000000) | 0.410000000000000 | Interval(445.000000000000, 452.000000000000) | . 101 Interval.Ropen(461.600000000000, 471.000000000... | 0.427872340425534 | Interval.Ropen(461.600000000000, 471.000000000... | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.open(478.000000000000, 490.000000000000) | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.open(491.000000000000, 505.200000000000) | . 102 Union(Interval.open(478.000000000000, 490.0000... | 0.273514431239389 | Interval.Lopen(508.750000000000, 512.000000000... | . 103 Interval(471.000000000000, 478.000000000000) | 2.37000000000000 | Interval(471.000000000000, 478.000000000000) | . 104 Interval(490.000000000000, 491.000000000000) | 8.15000000000000 | Interval(490.000000000000, 491.000000000000) | . 105 Union(Interval.Ropen(505.200000000000, 506.200... | 0.870491803278683 | Interval.Ropen(505.200000000000, 506.200000000... | . 105 Union(Interval.Ropen(505.200000000000, 506.200... | 0.870491803278683 | Interval.Lopen(506.700000000000, 508.750000000... | . 106 Interval(506.200000000000, 506.700000000000) | 100.480000000000 | Interval(506.200000000000, 506.700000000000) | . 107 Interval(600.000000000000, 620.000000000000) | 0.520000000000000 | Interval(600.000000000000, 620.000000000000) | . rig_exploded[&#39;From&#39;] = rig_exploded.split_intervals.apply(lambda x: x.start).astype(float) rig_exploded[&#39;To&#39;] = rig_exploded.split_intervals.apply(lambda x: x.end).astype(float) rig_exploded[&#39;Width&#39;] = (rig_exploded.To - rig_exploded.From).astype(float) rig_exploded[&#39;grade&#39;] = rig_exploded.grade.astype(float) rig_exploded[&#39;drillhole&#39;] = &#39;BR-022&#39; . rig_exploded[[&#39;From&#39;, &#39;To&#39;, &#39;grade&#39;, &#39;Width&#39;]] . From To grade Width . 95 110.00 | 111.40 | 0.572273 | 1.40 | . 95 113.10 | 116.10 | 0.572273 | 3.00 | . 96 111.40 | 113.10 | 7.920000 | 1.70 | . 97 274.00 | 299.00 | 0.190000 | 25.00 | . 98 435.70 | 439.00 | 0.545758 | 3.30 | . 99 432.90 | 435.70 | 8.180000 | 2.80 | . 100 445.00 | 452.00 | 0.410000 | 7.00 | . 101 461.60 | 471.00 | 0.427872 | 9.40 | . 102 478.00 | 490.00 | 0.273514 | 12.00 | . 102 491.00 | 505.20 | 0.273514 | 14.20 | . 102 508.75 | 512.00 | 0.273514 | 3.25 | . 103 471.00 | 478.00 | 2.370000 | 7.00 | . 104 490.00 | 491.00 | 8.150000 | 1.00 | . 105 505.20 | 506.20 | 0.870492 | 1.00 | . 105 506.70 | 508.75 | 0.870492 | 2.05 | . 106 506.20 | 506.70 | 100.480000 | 0.50 | . 107 600.00 | 620.00 | 0.520000 | 20.00 | . y_axis = alt.Axis( title=&#39;Intercept ID&#39;, offset=5, ticks=False, domain=False ) reqd_cols = [&#39;From&#39;, &#39;To&#39;, &#39;grade&#39;, &#39;Width&#39;, &#39;drillhole&#39;] alt.Chart(rig_exploded[reqd_cols]).mark_bar().encode( alt.X(&#39;From:Q&#39;, scale=alt.Scale(zero=False)), x2=&#39;To:Q&#39;, y=alt.Y(&#39;drillhole:N&#39;, axis=y_axis), color=alt.Color(&#39;grade:Q&#39;, scale=alt.Scale(scheme=&quot;inferno&quot;)), tooltip=[ alt.Tooltip(&#39;Width:Q&#39;, title=&#39;Width&#39;), alt.Tooltip(&#39;grade:Q&#39;, title=&#39;Gold Grade&#39;) ] ).properties(width=800, height=100).configure(background=&#39;#D9E9F0&#39;).interactive() .",
            "url": "https://nshea3.github.io/blog/2020/12/02/Mathematics-of-Drilling-Intercepts.html",
            "relUrl": "/2020/12/02/Mathematics-of-Drilling-Intercepts.html",
            "date": " â€¢ Dec 2, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Simple Betting Strategies in Python",
            "content": "Introduction . A student of probability is practically guaranteed to see games of chance during their studies - coin flipping, card drawing, dice games. This usually seems contrived, when in fact games of chance have been central to the development of probability and statistics. Unfortunately, the games studied are often simplified and betting strategies are constrained. . There are good reasons for this: analysis of betting strategies requires an understanding of simulation, which might be too advanced for an intro probability course. But at a certain point it&#39;s useful to analyze simple toy strategies and discover their extreme properties. The martingale system is one such strategy: a simple betting that recommends doubling the value bet with each loss. . It is not a new strategy, as we can gather from this 18th century account: . Casanovaâ€™s diary, Venice, 1754:Playing the martingale, and doubling my stakescontinuously, I won every day during the remainder of the carnival. [..] I congratulated myself upon having increased the treasure of my dear mistress [..] . Then, several days later: . I still played on the martingale, but with such bad luck that I was soon left without a sequin. As I shared my property with my mistress, [..] at her request I sold all her diamonds, losing what I got for them; she had now only five hundred sequins by her. There was no more talk of her escaping from the convent, for we had nothing to live on! . Source:Betting systems: how not to lose your moneygambling (G. Berkolaiko) . Casanova&#39;s account is important as it reveals some of the shortcomings of the strategy: the strategy does not consider the remaining bankroll, so the gambler with finite resources exposes themselves to the risk of ruin. In slightly more technical terms martingale betting strategy on a fair (50-50, even odds) game is a sure thing only with infinite capital to absorb long losing streaks with exponentially increasing losses. An unfair (subfair) game is a loss no matter the betting strategy (there is theoretical work to support this). Subfair games include most casino games (poker excepted - the casino will never let you pick their pocket but picking another punter&#39;s pocket is a matter of supreme indifference). . Roulette is one such subfair casino game. Roulette distinguished not only by its but its utility as a litmus for gamblers. Roulette strategies reveal bad gamblers (who favor magical thinking and lucky numbers), smart gamblers (who strictly use bold play on red or black), and smarter gamblers (to be found in the poker room or at a hedge fund). . Roulette, or rather Rou-let&#39;s not . The roulette wheel is likely familiar from any number of appearances in movies and TV. The pockets in the American roulette wheel are labeled with numbers 1 to 36, alternating red and black color. Two special pockets, numbered 0 and 00, are colored green. . . Boil it down to a choice, uniformly at random between the integers 1 to 38, inclusive. (notice that there is a 0 and a 00 that are neither Red nor Black). . There is a commom misconception that the machine needs to be somehow &quot;rigged&quot; or &quot;fixed&quot; (non-uniform random) to rob the player of their advantage. This is not true, the math works out that way for a random choice. The mechanism is completely random, but the house edge is guaranteed through careful construction of the wheel and the possible wagers. If you lose your shirt at the table, your gripe is with probability, not the casino or the gaming control board. . The house edge comes from a restriction on the possible bets and unfavorable payouts on the allowed bets. If I had my way, I would walk into the casino and wager all of my money against all of theirs that the outcome is in $ {00, 0, 1, 2, 3, ... , 36 }$ (with probability 1) and be guaranteed a hefty profit. Unfortunately, we can&#39;t do that. We cannot even come close: in American roulette, the best possible bet is any one of the . The first step in any simulation is to write code that handles the logic of the generating process itself. As we describe above, roulette is a simple uniform random choice among $n$ pockets, where each of the possible bets has a payoff that is easily calculated from the uniform probability law (a table of these odds and expected values can be found at https://en.wikipedia.org/wiki/Roulette#Bet_odds_table). So one very generic way to describe roulette spins is through a function that takes the calculated win probability and returns True if a random float is within the win probability or False if the random float is outside the win probability. . def get_spin_result(win_prob): result = False if np.random.random() &lt;= win_prob: result = True return result . So, if we bet on Black (the closest thing to a fair bet we&#39;ll find in this game), we double our money if the ball lands on Black and lose it all if the ball lands on Red. (a 1:1 payout) . Doubling Down . Now we&#39;ve established what happens in a single round of roulette, we need to ask how to make generalizations about the game: what exactly can we control in this game? . Type of bets | Value of bets | For simplification sake, we&#39;re considering only one type of bet (though the . This is where the martingale strategy comes in: . After every win: Reset the wager $w$ to some base wager $n$ . After every loss: Double the wager to $2w$ . So after some number of losses (let&#39;s say $k$ losses), the next wager will be $n*2^{k}$ . Now we have a (not) new, enticingly simple strategy, the first order of business is to determine the performance of this strategy in our game. . With some sloppy hardcoding, our simulation code is the follwing: . def sim1k(win_prob, n=1000): # This will be a list of True/False boolean spin outcomes wl_seq = [get_spin_result(win_prob) for sim in range(n)] # Use these to collect our winnings, and set our win amount and bet size winnings = [] win = 0 bet = 1 # Progress through win loss outcomes in wl_seq for wl in wl_seq: # Cut off when total winnings reach $80 if win &lt; 80: # Resetting bet to 1 in case of win if wl == True: win = win + bet bet = 1 # Doubling in case of loss else: win = win - bet bet = bet * 2 winnings.append(win) return winnings . win_prob = (18./38.) tentrials = [sim1k(win_prob) for x in range(10)] for trial in tentrials: plt.plot(range(1000), trial) . ktrials = [sim1k(win_prob) for x in range(1000)] #print(sum(np.any(np.array(ktrials) == 80., axis=1)) / len(np.any(np.array(ktrials) == 80., axis=1))) ktmean = np.mean(ktrials, axis = 0) ktsd = np.std(ktrials, axis = 0) for arr in (ktmean, ktmean + ktsd, ktmean - ktsd): plt.plot(range(1000), arr) . ktmed = np.median(ktrials, axis = 0) for arr in (ktmed, ktmed + ktsd, ktmed - ktsd): plt.plot(range(1000), arr) . So what&#39;s the problem there? . Do not track bankroll - we can make huge losses during the interim . In any reasonable casino, there is a limit to bet size . Often the lower ceiling is the size of our own bank account . def sim1k_bankroll(win_prob, n=1000): wl_seq = [get_spin_result(win_prob) for sim in range(n)] winnings = [] win = 0 bet = 1 for wl in wl_seq: if (win &lt; 80 and win &gt; -256): if wl == True: win = win + bet bet = 1 else: win = win - bet bet = min(bet * 2, 256. + win) winnings.append(win) return winnings . ktrials_real = [sim1k_bankroll(win_prob) for x in range(1000)] ktmean = np.mean(ktrials_real, axis = 0) ktsd = np.std(ktrials_real, axis = 0) ktmed = np.median(ktrials_real, axis = 0) plt.figure() plt.xlim((0,300)) plt.ylim((-256,100)) for arr in (ktmean, ktmean + ktsd, ktmean - ktsd): plt.plot(range(1000), arr) . for arr in (ktmed, ktmed + ktsd, ktmed - ktsd): plt.plot(range(1000), arr) . What if I HAVE to play roulette? . First, my condolences to you and your wallet! . Fortunately, all is not lost. There is a strategy that can help you leave with your shirt. The intuition is explained in Berkolaiko&#39;s lecture: . If the game is subfair, every round we lose some (on average). . To minimize losses â€” minimize rounds! . Although we are in &quot;damage control&quot; mode, we are still being exposed to randomness that can cut both ways. In the long run, it will cut against us, but we can choose to simply not expose ourselves to the long run. We will bet aggressively and get out as soon as possible. . Conclusions . Sources . Betting systems:how not to lose your moneygambling (Gregory Berkolaiko) . Expected value and Betting systems (Luc Rey-Bellet) . Analysis of a Betting System (Rabung and Hyland) . A Statistical Analysis of the Roulette Martingale System: Examples, Formulas and Simulations with R (Peter Pflaumer) .",
            "url": "https://nshea3.github.io/blog/python/simulation/betting/strategy/martingale/2020/11/20/Simple-Betting-Strategies-Python.html",
            "relUrl": "/python/simulation/betting/strategy/martingale/2020/11/20/Simple-Betting-Strategies-Python.html",
            "date": " â€¢ Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "See nshea3.github.io [^1]. . I am a data scientist and engineer with BlueConduit, where I work to secure a clean water future $ forall$. . I am interested in spatial statistics, simulation, interpretable ML, and Bayesian methods, especially applications in natural resources, social science, and financial markets. .",
          "url": "https://nshea3.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://nshea3.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}