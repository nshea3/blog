{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-19-Tedium-Free-MLE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHjCFpvxRY7h"
      },
      "source": [
        "\n",
        "# Tedium Free MLE\n",
        "\n",
        "Maximum likelihood estimation has the dubious honor of being difficult for humans and machines alike (difficult for machines at least in the naÃ¯ve formulation that doesn't use log-likelihood).\n",
        "\n",
        "MLE is challenging for humans  because it requires the multiplication of $n$ likelihood expressions, which is time consuming and error prone - this is the *tedium* part we're trying to avoid. Fortunately, computers are very good at repeated multiplication, even repeated *symbolic* multiplication. \n",
        "\n",
        "## Problem Formulation and Example\n",
        "\n",
        "MLE estimates parameters of an assumed probability distribution, given data $x_i$ observed independently from the same distribution. If that distribution has probability function $f(\\cdot)$, then the likelihood of $x_i$ is $f(x_i)$.  \n",
        "\n",
        "As the $x_i$s are independent, the likelihood of all $x_i$s will be the product of their individual likelihoods. In mathematical notation, the product will be: \n",
        "\n",
        "$$\\prod_{i=1}^{n} f(x_i)$$\n",
        "\n",
        "Probability functions (mass functions or density functions) like our $f(\\cdot)$ typically have **parameters**. For instance, the Gaussian distribution has parameters $\\mu$ and $\\sigma^2$, and the Poisson distribution has rate parameter Î». We use MLE to estimate these parameters, so they are the unknowns in the expression and they will appear in each $f(x_i)$ term. We can restate the problem as an equality with the generic parameter $\\theta$:\n",
        "\n",
        "$$L(\\theta) = \\prod_{i=1}^{n} f(x_i)$$\n",
        "\n",
        "The expression $L(\\theta)$ is the likelihood. In order to find the MLE it is necessary to *maximize* this function, or find the value of $\\theta$ for which $L(\\theta)$ is as large as possible. This process is probably easier to show than to describe. In particular, we'll be demonstrating the usefulness of the `sympy` module in making these symbolic calculations.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Say we observed values $[3,1,2]$ generated from a Poisson. What is likelihood function of Î»?\n",
        "\n",
        "Importing the necessities and setting up some symbols and expressions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5TD8BoP9lO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "95767e2d-1fb6-41e5-ad5c-4d89b490c16d"
      },
      "source": [
        "from sympy.stats import Poisson, density, E, variance\n",
        "from sympy import Symbol, simplify\n",
        "from sympy.abc import x\n",
        "\n",
        "lambda_ = Symbol(\"lambda\", positive=True)\n",
        "\n",
        "f = Poisson(\"f\", lambda_)\n",
        "density(f)(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\frac{\\lambda^{x} e^{- \\lambda}}{x!}$",
            "text/plain": [
              "lambda**x*exp(-lambda)/factorial(x)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZIC-xUlCeG_"
      },
      "source": [
        "`sympy` gives us a representation of the Poisson density to work with in the [`Poisson()` object](https://docs.sympy.org/latest/modules/stats.html#sympy.stats.Poisson), keeping track of all of the terms internally. \n",
        "\n",
        "The likelihood expression is the product of the probability function evaluated at these three points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "B7tli1TYTfKe",
        "outputId": "4ecc3b5e-8d4b-4170-c9ad-1b2bf726526a"
      },
      "source": [
        "L_ = density(f)(3) * density(f)(1) * density(f)(2)\n",
        "L_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle \\frac{\\lambda^{6} e^{- 3 \\lambda}}{12}$",
            "text/plain": [
              "lambda**6*exp(-3*lambda)/12"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95LPdYfL8iJw"
      },
      "source": [
        "That's our expression for the likelihood $L(\\theta)$ ðŸ™‚ In order to maximize the expression, we'll take the derivative expression and then solve for the value of parameter $\\lambda$ where the derivative expression is equal to 0. [This value of $\\lambda$ will maximize the likelihood.](https://tutorial.math.lamar.edu/classes/calci/DerivativeAppsProofs.aspx)\n",
        "\n",
        "Finding the derivative using `sympy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cnA5Cz388_HU",
        "outputId": "b889cf6d-bc5b-41f3-a6e3-9b444e8c5e46"
      },
      "source": [
        "from sympy import diff\n",
        "\n",
        "dL_ = diff(L_, lambda_)\n",
        "dL_"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle - \\frac{\\lambda^{6} e^{- 3 \\lambda}}{4} + \\frac{\\lambda^{5} e^{- 3 \\lambda}}{2}$",
            "text/plain": [
              "-lambda**6*exp(-3*lambda)/4 + lambda**5*exp(-3*lambda)/2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVNIMrfk9JxQ"
      },
      "source": [
        "Setting the derivative $\\frac{dL}{d\\theta}$ equal to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "l4otgAyh9Hvk",
        "outputId": "d7d8f470-3556-4911-c98b-656faed567b2"
      },
      "source": [
        "from sympy import Eq\n",
        "\n",
        "\n",
        "dLeqz = Eq(dL_, 0)\n",
        "dLeqz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle - \\frac{\\lambda^{6} e^{- 3 \\lambda}}{4} + \\frac{\\lambda^{5} e^{- 3 \\lambda}}{2} = 0$",
            "text/plain": [
              "Eq(-lambda**6*exp(-3*lambda)/4 + lambda**5*exp(-3*lambda)/2, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNi803AzBKyE"
      },
      "source": [
        "And finally, solving the equation for $\\lambda$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rm0Lq3M9rWr",
        "outputId": "0e908914-78de-4772-833e-56055350bb8d"
      },
      "source": [
        "from sympy import solve\n",
        "\n",
        "solve(dLeqz, lambda_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHhNQPTCToiP"
      },
      "source": [
        "And that's our answer! \n",
        "\n",
        "There is a slight wrinkle with this approach. It is not widely this approach, MLE can be difficult for computers too. \n",
        "\n",
        "Likelihoods are usually very small numbers and computers simply can't track numbers that are too small or too large. Multiplying very small numbers together repeatedly makes very VERY small numbers that can sometimes disappear completely. Without getting too distracted by the minutiae of numerical stability or underflow, we can still appreciate some bizarre behavior that results when floats are misused:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmpKQDVHDGi_",
        "outputId": "de1d7123-f45f-4e02-bc38-ce7a11493e7f"
      },
      "source": [
        "6.89 + .1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.989999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8h1El1MDdJ8",
        "outputId": "df201b51-0ceb-4f8b-8ad8-0704bf917183"
      },
      "source": [
        "(0.1)**512"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6bznxj8FjdK"
      },
      "source": [
        "In the second scenario, we can imagine having 512 data points and finding that the likelihood evaluates to 0.1 (times our parameter) for every single one. Then our product would look like $g(\\theta) \\cdot (0.1)^{512}$. The computer just told us that one of those terms is *zero*, and we're left unable to find the parameters for our MLE. \n",
        "\n",
        "What do we do instead? Is there any way to make these numbers bigger, without changing the problem or solution? Is there an equivalent problem with bigger numbers?\n",
        "\n",
        "Adding a number and multiplying by a number don't fix the problem - they just add terms to the expression, which ends up zero anyhow. However these functions do have one property that we will need to be sure we are solving an equivalent problem: *they preserve the order of the input in the output.* We call these functions **monotonic**.\n",
        "\n",
        "The monotonic functions also include the *log* function. The log function has some very nice properties, not least of which that it makes our calculations immune to the problems we saw above. Calculating the log likelihood:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "sYcHWrpGTnul",
        "outputId": "bdf38945-16e0-4371-9a10-35cd0b92bb73"
      },
      "source": [
        "from sympy import log\n",
        "\n",
        "_ = simplify(log(L_))\n",
        "_"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle - 3 \\lambda + 6 \\log{\\left(\\lambda \\right)} - \\log{\\left(12 \\right)}$",
            "text/plain": [
              "-3*lambda + 6*log(lambda) - log(12)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSb_abIxEshX"
      },
      "source": [
        "And then taking the derivative as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNzrGJ_VRNb"
      },
      "source": [
        "d_ = diff(_, lambda_)\n",
        "d_"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-6Npz_FEzw1"
      },
      "source": [
        "Setting equal to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "_9pK5HbgUZbD",
        "outputId": "73779fba-4764-4c0e-9a7b-e16eab06695e"
      },
      "source": [
        "_ = Eq(_, 0)\n",
        "_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/latex": "$\\displaystyle -3 + \\frac{6}{\\lambda} = 0$",
            "text/plain": [
              "Eq(-3 + 6/lambda, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcmlEGYE2Rg"
      },
      "source": [
        "And solving:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdzQgKlhUCbF",
        "outputId": "a8381761-8478-4f05-ac31-853042898884"
      },
      "source": [
        "from sympy import solve\n",
        "\n",
        "solve(_, lambda_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fze7gQRE3t7"
      },
      "source": [
        "The two solutions agree! Which is necessary, but not sufficient to show these methods are equivalent in general. "
      ]
    }
  ]
}